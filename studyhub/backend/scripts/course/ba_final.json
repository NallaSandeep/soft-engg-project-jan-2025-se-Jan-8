{
    "course": {
        "course_id": 4,
        "code": "BA201",
        "title": "Business Analytics",
        "description": "A comprehensive course providing in-depth coverage of data visualization techniques, statistical distribution fitting, association analysis, Bayesian inference, and advanced demand modeling for informed business decision-making.",
        "instructor_id": 1002,
        "credits": 4,
        "department": "Business Analytics",
        "image_url": "/assets/courses/ba201/cover.jpg",
        "prerequisites": ["None"],
        "learning_outcomes": [
            "Master data visualization principles and effective chart selection",
            "Analyze and fit probability distributions to business data",
            "Perform statistical association analysis and chi-square testing",
            "Apply Bayesian inference for decision-making under uncertainty",
            "Model demand response curves and estimate price elasticity"
        ],
        "assessment_methods": ["Quizzes", "Assignments", "Final Exam"],
        "delivery_mode": "Online",
        "tools_and_technologies": ["Excel", "R", "Python"],
        "LLM_Summary": {
            "summary": "The Business Analytics course (BA201) offers a robust exploration of analytical techniques essential for data-driven decision-making. The course delves into data visualization best practices, ensuring accurate representation and communication of insights through effective chart selection and design. It thoroughly covers the fitting of probability distributions to business data, including parameter estimation using Maximum Likelihood Estimation (MLE) and graphical methods such as Q-Q plots. The course also emphasizes statistical association analysis through methods like correlation metrics and chi-square testing. Bayesian inference is introduced for probabilistic reasoning under uncertainty, providing a framework to update beliefs using new evidence. Furthermore, advanced demand modeling techniques are explored, focusing on linear and non-linear demand curves, elasticity estimation, and interpreting the relationship between price changes and consumer behavior. Real-world case studies and hands-on exercises reinforce these methodologies, preparing students to apply analytical frameworks to practical business challenges.",
            "concepts_covered": [
                "Data visualization principles and chart selection",
                "Effective communication of analytical insights",
                "Probability distributions (theoretical and empirical)",
                "Parameter estimation using Maximum Likelihood Estimation (MLE)",
                "Graphical techniques (Q-Q plots) for distribution validation",
                "Goodness-of-fit testing (Chi-square, Kolmogorov-Smirnov)",
                "Statistical association and correlation metrics",
                "Chi-square analysis for independence testing",
                "Bayesian probability and belief updating",
                "Application of Bayes' theorem in decision-making",
                "Demand response curves and elasticity estimation",
                "Linear and constant elasticity demand models",
                "Experimental design for market data analysis",
                "Case studies on data-driven business decisions"
            ],
            "concepts_not_covered": [
                "Machine learning algorithms and predictive modeling",
                "Big data processing technologies (e.g., Hadoop, Spark)",
                "Database design and query optimization",
                "Cloud-based analytics platforms",
                "Advanced time-series forecasting",
                "Causal inference methods in experimental design"
            ]
        }
    },
  
    "weeks": [
    {
      "week_id": 1,
      "course_id": 4,
      "order": 1,
      "title": "Week 1: Data Visualization",
      "estimated_hours": 32,
      "LLM_Summary": {
        "summary": "This week provides a comprehensive foundation in data visualization principles and techniques, covering both theoretical frameworks and practical applications. The material emphasizes how visualization serves as a critical bridge between data analysis and business decision-making. Key topics include the four core principles of effective visualization (data integrity, purpose-driven design, data-ink ratio optimization, and strategic annotation), along with detailed guidance on selecting appropriate chart types for different data scenarios. The week explores cognitive aspects of visual perception, dashboard design best practices, and common pitfalls in business presentations. Advanced concepts include the psychology of color in data representation, interactive visualization techniques, and the role of visualization in exploratory data analysis. Real-world case studies demonstrate how effective visual communication can transform raw data into actionable business insights.",
        "concepts_covered": [
            "Fundamental principles of effective data visualization",
            "Data integrity and avoiding misleading representations",
            "Purpose-driven visualization and audience-centric design",
            "Optimizing data-ink ratio for clarity",
            "Effective annotation techniques",
            "Visual perception attributes (form, color, spatial positioning)",
            "Dashboard design principles",
            "Chart selection for different data types",
            "Common visualization pitfalls and remedies",
            "Cognitive aspects of visual communication"
        ],
        "concepts_not_covered": [
            "Advanced interactive visualization techniques (D3.js, Plotly)",
            "Geospatial visualization and mapping",
            "3D data visualization methods",
            "Real-time streaming data visualization",
            "Visualization for big data architectures",
            "Augmented reality data representation",
            "Advanced dashboard interactivity",
            "Visual storytelling techniques",
            "Visualization in machine learning pipelines",
            "Accessibility in data visualization"
        ]
      }
    },
    {
      "week_id": 2,
      "course_id": 4,
      "order": 2,
      "title": "Week 2: Distribution Fitting",
      "estimated_hours": 15,
      "LLM_Summary": {
        "summary": "This week delves into probabilistic modeling through distribution fitting techniques, covering both theoretical foundations and practical implementation. The material explains how to characterize business data using appropriate probability distributions, including both standard parametric models (normal, exponential, uniform) and empirical approaches. Detailed methodologies are provided for parameter estimation (including Maximum Likelihood Estimation), goodness-of-fit testing (chi-square, Kolmogorov-Smirnov), and visual assessment techniques (Q-Q plots, P-P plots). The week emphasizes the business applications of distribution fitting in areas like risk assessment, forecasting, and simulation modeling. Advanced topics include handling skewed distributions, mixture models, and the relationship between distribution fitting and predictive analytics. Practical examples demonstrate how to select the most appropriate distribution for given datasets and validate model assumptions.",
        "concepts_covered": [
            "Probability distribution fundamentals",
            "Discrete vs. continuous distributions",
            "Theoretical distribution fitting (normal, exponential, uniform)",
            "Empirical distribution methods",
            "Parameter estimation techniques",
            "Goodness-of-fit tests",
            "Probability plot interpretation",
            "Business applications of distribution fitting",
            "Trace-driven vs. model-based simulation",
            "Handling grouped and censored data"
        ],
        "concepts_not_covered": [
            "Multivariate probability distributions",
            "Bayesian approaches to distribution fitting",
            "Non-parametric density estimation",
            "Time-dependent distributions",
            "Extreme value distributions",
            "Copula models for dependence structures",
            "Markov chain Monte Carlo methods",
            "Distribution fitting for high-dimensional data",
            "Machine learning approaches to distribution estimation",
            "Distribution fitting in quality control (Six Sigma)"
        ]
      }
    },
    {
      "week_id": 3,
      "course_id": 4,
      "order": 3,
      "title": "Week 3: Association Analysis",
      "estimated_hours": 18,
      "LLM_Summary": {
        "summary": "This week focuses on statistical methods for analyzing relationships between variables, with particular emphasis on categorical data analysis. The material covers contingency table analysis, chi-square tests of independence, and Bayesian probability updating. Detailed explanations are provided for joint, marginal, and conditional probabilities, along with their business applications in market segmentation, quality control, and risk assessment. The Bayesian analysis component demonstrates how to systematically update prior beliefs with empirical evidence, including practical techniques for prior specification and posterior interpretation. Advanced topics include measures of association strength (odds ratios, relative risk), Simpson's paradox, and the integration of frequentist and Bayesian approaches. Real-world case studies illustrate how association analysis can reveal hidden patterns in business data and support evidence-based decision making.",
        "concepts_covered": [
            "Contingency table analysis",
            "Chi-square test of independence",
            "Joint, marginal, and conditional probabilities",
            "Bayesian probability updating",
            "Prior and posterior distributions",
            "Likelihood functions",
            "Measures of association strength",
            "Business applications in marketing and operations",
            "Hypothesis testing framework",
            "Expected vs. observed frequencies"
        ],
        "concepts_not_covered": [
            "Log-linear models for multi-way tables",
            "Structural equation modeling",
            "Latent class analysis",
            "Bayesian networks",
            "Association rule mining",
            "Correspondence analysis",
            "Multilevel modeling of associations",
            "Causal inference techniques",
            "Propensity score matching",
            "Survival analysis with covariates"
        ]
      }
    },
    {
      "week_id": 4,
      "course_id": 4,
      "order": 4,
      "title": "Week 4: Demand Response Curves",
      "estimated_hours": 40,
      "LLM_Summary": {
        "summary": "This week provides a comprehensive treatment of demand modeling techniques, focusing on price elasticity estimation and revenue optimization. The material covers both fundamental concepts (demand curve properties, consumer surplus) and advanced modeling approaches (linear regression, constant elasticity models). Detailed methodologies are presented for experimental design, data transformation (log-log), and model validation. The week emphasizes practical business applications in pricing strategy, market segmentation, and revenue management. Advanced topics include discrete choice models, price optimization under constraints, and the integration of cost structures into demand modeling. Case studies demonstrate how to estimate and interpret price elasticity in various industry contexts, and how to use these insights for profit-maximizing decision making. The material also covers special cases like Veblen and Giffen goods, and their implications for conventional demand theory.",
        "concepts_covered": [
            "Demand curve fundamentals and properties",
            "Price elasticity concepts and measurement",
            "Linear demand models",
            "Constant elasticity models",
            "Log-log transformations",
            "Consumer surplus calculations",
            "Revenue vs. profit optimization",
            "Experimental design for demand estimation",
            "Business applications in pricing strategy",
            "Special cases (Giffen goods, luxury items)"
        ],
        "concepts_not_covered": [
            "Dynamic pricing models",
            "Conjoint analysis for demand estimation",
            "Discrete choice models (logit, probit)",
            "Price optimization under supply constraints",
            "Competitive demand modeling",
            "Behavioral economics aspects of demand",
            "Demand forecasting with external factors",
            "Multi-product demand interdependencies",
            "Personalized pricing models",
            "AI-driven demand prediction"
        ]
      }
    }
  ],
  "lectures": [
    {
      "lecture_id": 1,
      "week_id": 1,
      "order": 1,
      "title": "Principles of Effective Data Visualization",
      "resource_type": "youtube",
      "video_url": "https://www.youtube.com/watch?v=_0z2c-Awpt0",
      "content_transcript": "ood morning everyone, welcome to this session on data visualization. We are lucky to have Annad Srinivasan with us today, joining in from Bangalore. Anand is an IITM alumnus B. Tech 1995 is that correct Anand, yes and he has been a very amazing data scientist throughout his 26, 27 year old career has worked with multiple industries Airline industries, Saber technologies, Dell. He was with the senior management at Go Air and now he is about to start a new venture, a new airline which is going to be launched very soon. I can say that formally right Anand, yes you can say that formally Rahul, perfect. So, very glad that recently Anand was listed as one of the top 10 data scientists in India. So, very lucky to have you Anand on joining us today for the session, my pleasure Rahul it is always good to touch base with you and happy to help in any way I can. So, today's session is going to be on how you see data that is going to be the primary purpose. How do you represent data and most of the ideas are essentially Anand's idea that we are going to discuss and therefore we have Anand in the session today. So, let us start. So, first of all, why do you think we need to emphasize on visualization of data? So, you know there is a cliche right I mean a picture being worth a thousand words fundamentally that still holds true. End of the day when you look at analytics as a whole the fundamental purpose is you know people talk about data science etcetera. The only reason that field exists is to help businesses make better decisions, correct. If you count that then I mean that you question the very we need for the analytics function as a whole right. So, I have always been an exponent of saying look, it does not matter what kind of decisions are based on whatever analysis. Part of that is to say why we might build all kinds of sophisticated models you have to understand that decision makers you have to be able to present them that result of that analysis not necessarily the process but the result of that analysis in a very precise concise and consumable format which will go back and say hey if I do this this will this is likely to happen is this a good decision or a bad decision, correct. I think so, to me that is the crux of the problem right I mean let us face it in the forest and nobody heard it. So, you might have the greatest analysis but if good decisions are not being made based on that analysis then really what is the point of doing that analysis. So, I think that is where the visual ability to communicate and communicate visually to decision makers is a very, very critical part of analytics. In fact, that is my pet peeve if you want to call it that. It is like you know a lot of people that talk about analytics or you know people that teach analytics they do not emphasize this component enough. And people who have been endless will tell you how long the number of times come out to me after a long meeting with senior senior management and then come back and say oh we had this great model we had this great analysis but somewhere the whole discussion got derailed into something else and people did not really understand the core of what it is. And we ended up wasting time discussing some trivial aspects which were not core to the analysis that was presented all the time. And part of that is it is not that the management is sitting there saying look let us have chai samosa pakora and let us just white time away that they are there for a reason they want to understand the issue they want to understand the solutions. They want to make decisions but somewhere there the analyst has to take responsibility to communicate that effectively and I think that is where visualization comes in in a very, very critical way. So, I think I cannot emphasize the importance of visualization enough. So, in fact I am very happy Rahul that you are touching upon this as part of the course because I am not seeing this exercise because they want to quickly jump into statistical modeling recreation models and more importantly somewhere you know someone seems to how quickly can we get to AI and then I think emphasizing the fundamentals is so, critical lovely to see that, thank you. And not only for the corporate managers right, I mean I think psychologically also human beings are tuned to see pictures better than words right, yes absolutely absolutely right. In fact that is the thing right I mean this is well known right vision is the most powerful sense that we have there right. In fact I think there have been studies that talk about cognitive studies right 70% of the information we consume is visual right auditory sensory touch. So, I mean you know people can sit and debate the 70% 30% you know like I always say right 85% of statistics are made up on the fly right. But that being said, I do not think so. I think it is beyond debate that vision happens to be the most powerful perception that we have. So, which is why visual communication is so important right and that is what we will talk about as we talk through some of these things, perfectly. And it doesn't really matter what kind of data you have, data can be numerical data can be continuous, it can be discrete and we have visualization tools right. We have visualization tools for each data type that I mean really there is no dearth of visualization techniques. So, interesting that your choice of words I mean I would emphasize the techniques more than the tools right. You know I think people once you understand the techniques you understand the fundamentals the tool is just a medium right. So, when you say visualization tool maybe perspective I am thinking of software tools that help you do visualization that is not the issue, correct. So, I would emphasize techniques more than tools because really what we want to teach people are the techniques. Then the tool just becomes a means of implementing that technique correct correct that is that is we wish to this that is what we wish to discuss in the session today I mean essentially what are the what are the core principles on which the visualization should be based irrespective of what data do you have that you want to represent. And you know what the other thing is. I think you know people need to understand these kinds of categorical numerical discrete continuous data. Because the very nature of the data right dictates how you represent them visually okay, correct. The number of times I have seen people show me you know trending data using categorical data, right. So, that is the whole point right you know if you are going to show me say a line chart which makes sense for you know time continuous trending kind of a data. But then you decide to draw a line chart on categorical data somewhere there you know when you say okay between this point and this point you have drawn a line connecting these two points which is exactly between these two, right. So, and you know and these kinds of things the amazing thing is you know when you when you actually talk about it you know everybody will smile and say yeah come on I mean like obviously is not that obvious part of the funny part depending the so on your point of view is the fact that it happens so frequently okay uh. So, I think it is just like you know part of it is tools that is the reason why I emphasize the difference between tools and techniques because what happens is you pull up the data you go to excel or something and then draw click and say create chat it just it just it is a it is a dumb tool right it just does not work. So, you know spending that little bit of time and hopefully the points we consider today when we discuss will get people to think for you know two seconds before creating a chart, right. And the two seconds will save you two hours of debate time when you are presenting your results precisely, yeah yeah yeah yeah yeah yeah. So, what do you think are the general benefits of visualizing our visual representation of the data? See benefits see the whole idea is communication right I mean there is only one reason why look if you think about it business has to improve. So, we do analysis etc to help the business improve and we need to communicate that to the stakeholders. So, the right decisions can be made very simply. Our objective as analysts is to communicate that idea. So, visualization is our language of; so I could almost call visualization the language of the analyst because let us face it you go sit in a boardroom they are not going to be interested in looking at your python code, yeah, correct. So, visualization is the language of analysts when they talk to the business stakeholder right. Now and by the way interestingly visualization is also the language by which the analyst understands the real world. Because the right visual representation can actually give you a very good view of what could be the core problem and then actually put you on the right path to solving the problem, yeah. If you do not have a good visual representation the problem itself may not necessarily be evident. So, when you say it is visual the stuff you talk about cognitive processes here yes that is that that is what the communication is visual representation visual communication. And obviously communication is to make the cognitive process easier. So, part of what we do visualization is to say look, the other person has to understand the concept and we have to make it easier for them to understand the concept, not harder right. There are times when the visual representation itself is. So complicated that you know people spend more time understanding the visual than what it is trying to communicate, yeah yeah yeah. So, I call that the equivalent of going to a foreign country and saying you know where the washroom is and the person gives you an answer, right. You spend more time trying to understand what the person was saying rather than where the washroom is, yeah. It might be just easier for the person to point, yeah yeah yeah. So, you know, having the wrong visualization is the equivalent of getting a very simple answer in a completely spend foreign language. If you take more time understanding what the person will say you know and rather than saying okay look I asked you for the washroom you pointed me there problem solved this is thank you, correct correct correct correct. And I mean I have experienced this also I mean. So, many times we take up a constraint assignment and at the end of the assignment you are supposed to make a presentation to the senior management. And uh, gently the person comes to you before the meeting and says boss do not present your math and do not present your code. Tell us what it means to us, right?'' So, unless we speak the language that people understand. So that they can make better decisions about our model and our code is useless, absolutely right. And you know and you know see someone's right I mean which is kind of straying away from the core concept of what we are talking about here. I think as engineers right we also fall into the trap of confusing effort for importance right yes. So, you know you might have spent 50% of your time right cleaning up the data and lining up the data correctly and maybe only 30% actually building the model or you know or say let us say 60, 70-30 70% and you know typical 70 or you will spend you know just cleaning up the data understanding this. That does not mean you know that your process of cleaning up the data has to consume 70% of your time; the time of your final presentation, yeah right. So, so. so we do fall into the trap of mistaking effort for importance, right. So, it does happen. So, that is part of the visualization is to say look but I think that is slightly outside the scope of what we are trying to discuss here but you know also be cognizant of that. So, I think when we talk about this we will talk about you know what is the purpose of this presentation. So, maybe some of those things will be touched upon as we go through, correct, correct. So, essentially by visualization we mean visually highlighting a few things for example through the form or through color or through spatial representations right, yeah. So, see when you try to build a visual representation right and these are all I mean if you really think about it all charts that you do use one of these techniques to and these are the things that the human eye spots very easily correct. So, for instance right I mean if you look at it we are trying very easily to kind of get a sense of identifying the shortest bar in those four lines right. I do not have to tell you that look mark 2o is shorter right. We are I just by looking at it I know correct the amazing thing is we are also very good at very quickly evaluating by how much it is shorter I mean somebody will look at this and say I would say it is about 15 shorter okay it is I mean you it may be 15 18 14.25 but bottom line is we without saying anything I look at it I get a very good idea, righ. Similarly the width right I mean you know when you highlight it you know when you use bold text stresses of normal text. Again we pick up on that very, very quickly right of course orientation size shape you know and obviously using enclosure to highlight which is you know when you draw a person. So, these are all attributes of visual perception and the whole idea is when you do. When you communicate you have to think about which of these can I use to draw the user's attention to the important part. How do you get them to focus on what is important and how do you get them to not focus on what is not important, correct uh. So, like they say right, what you say is as important as what you do not show or what you show is as important as what you do not show. You know I might have made a crack about you know beachwear but maybe not appropriate for the current audience. So, we leave that as it is, yeah okay. So, okay now let us get down to the core of what we are trying to say right. You have often spoken about the four important kinds of umbrella principles of visualization, Can you elaborate on that right. So, I mean something that I mean you know I honestly speaking unfortunately I would like to know if I could I would go back and say that did I get this idea from uh. So, at the risk of saying you know it was pleasing I was inspired right at some point I might have seen this somewhere but I kind of it resonated. So, I kind of made it a cornerstone of a lot of times when I talk to people in terms of this. So, if there is somebody out there who actually came up with these things you know, consider the credit date granted, right right. So, the four things I always talk about is when you present a chart when you build this one. These are four things that you have to absolutely follow and in fact write it down and go through a checklist. First is to know the purpose. What is the purpose of putting this visual representation together, right? It has to have a purpose it is and the purpose cannot be you know let me demonstrate my mastery of the charting tool, okay. So, the purpose is why does this graph have to be here? Why does this chart have to be here? Why does this representation have to be here? I want to use the word graph chart and visual representation interchangeably but you know sometimes in cases there could be different. Once you have a purpose correct that purpose will automatically determine what form of representation it will do, correct. Second, I always ensure the integrity of what you represent, right? And you know typically you will have errors of omission and commission when you are presenting data, right. To me that is a non-negotiable right integrity of the data because even if it is by omission or commission. Even if there is a small error in the data correct it will and that could be in the most trivial you know sidebar after this one but it will derail the entire analysis and essentially you know even if it was oh you know it is actually something I made a typo and I put it into this presentation, correct. Once it gets caught the integrity of the entire presentation will be questioned. So, integrity is absolutely critical and it is non-negotiable. correct and you know I always talk about you know data inc and minimizing non-data which is the equivalent of saying you know what spend ink on the items that you want to show do not spend ink on items that you it is not which is not critical to the thought process. So, maybe just moving on will show your data and annotate yourself with the integrity of it as well, right. So, let us take this one by one uh. So, let us understand what we mean by purpose right. So, by purpose we mean the actual business problem that we are trying to solve, right. So, no, not really necessarily Rahul. So, because see if I build a chart or a visual right it is a one step in the larger question that I am trying to answer. So, which means that this is because let us face it right, the business problem I am trying to solve is not going to be solved by one visual representation, right yeah. I might be going through a slide and you know a series of slides, this one the purple. When I put a representative graphic together the purpose of that should say this is going to make my ability to communicate this more complicated concept of this one is that much easier it sets the stage in some cases or this might emphasizes this, correct, okay. Another way to do it is if you look at even this particular graphic that we are seeing on the screen, correct. Because I always talk about the umbrella principles, right. What do I mean by umbrella principles in English? It covers everything, correct, yeah. And if you look at that little handle that I have shown there, if I go back and say what the purpose of having that handle is, it emphasizes the concept of the umbrella principle, yeah correct. So, it is there for a reason, correct. So, which means literally everything that there has to be a reason. So, if you look at it and say why is this there, what is the purpose of having that particular graphic on the screen? You should have a clear purpose, right. Now the purpose again the reason I say a purpose is not necessarily the message what we are talking about how we communicate the message, right. The message could be communicated over a series of visual representations, okay and each of those will have a purpose that contributes towards communicating that message, okay okay okay, right. So, understood and in fact when people are starting off I always encourage you to whenever you put this together right what is the purpose? Does it serve a purpose? If it does not serve a purpose remove it. Sometimes and you know there is one thing to say it does not remove it sometimes certain visual objects will actually have a counter purpose, correct, yeah yeah. So, you have to be very careful and focus on what you want to say correctly. So, I always encourage you to know when you are putting thought through and say that what is the purpose of having this visual representation, this slide, this graphic whatever it may be, right. But just have that in fact I used to tell people to write it down, right saying what is the purpose right. So, nice, correct, correct . I understand the difference between the message that we want to convey and the purpose for which this visual tool is being used. I understand, exactly, exactly , exactly , right , okay. Second thing was integrity yeah and this I can see all all too often right. It is pretty immediate. So, for instance right I mean you know just to give an example you should not be presenting in a way to destroy right. So, if you look at the graphic on the right that shows you, let us say typically earnings per share is this one. Now here is the beauty of it right. When you look at the graph on the left on the graph on the right without paying attention the first thing that one would say is that oh my god the when I see the graph on the left it is. So, volatile, yeah right in reality it is not correct. I mean look if somebody pulls out a calculator and says okay 164, 220 what is the percentage but remember the thing is about how usually we see length and we are very quick to measure. So, when I look at the 1999 versus 2000 right on the left graph one would say we grew six x or five almost yeah yeah correct right that is that was not the reality right because what somebody has done is you can see the access has been cut off, correct, yeah. Now and in fact when I say this right somebody will come back and say no no I want to emphasize the variability, correct. Now and that is where I say it is being the integrity is lost because the reality is the variability is minuscule, correct yeah yeah. We are exaggerating on the left hand side yeah. So, by chopping it off; so, a lot of people say no I want to emphasize the difference or the variability I said in the grand scheme of things the variability is nothing. So, they will say I want to show that north is so much better than south or sales in east are so much bigger. But in reality the per capita might be you know off by you know 0.5% but by chopping it off I will make it up here it is off by you know 30 40%, correct. um This is you will be; I am amazed at how frequently this is used especially in media exactly. I was going to say that, yes, okay. And their equivalent of caviar temper is they will have a small little wiggly thing at the bottom to show that access has been cut, yeah yeah yeah. Right. And you know and when I see this graph that is the first thing I notice right the small wiggly squiggly line at the bottom yeah yeah. So, this is very common in media right and you know when you look at it then you and amazingly enough when you look at it from the lens of the integrity is being gone you actually will understand the bias of the storyteller in terms of what they are trying to say here, yeah yeah yeah that could become very evident, right. So, uh. So, so to me how do you eliminate as a storyteller as a media person I need to be making a statement of fact right by chopping it off you are not stating fact right. So, to me integrator integrity is so critical right I mean I mean and I chose the word the left the graph on the left is deceitful it is a very strong word and I chose it deliberately because I believe it is deceitful it is communicating our wrong picture, precisely, yeah. But the sad reality is exactly what you said you see every day in print media in the news media in the electronic media you see this every day yes and you know the most common technique used, right. So, again as um analytics analytics professionals correct and when we are presenting energy it is our responsibility to make sure these kinds of things do not happen, precisely yeah yeah yeah. Now moving on to the third thing about maximizing what we wish to highlight on the data inc, yeah. So, again, right , very simple. I am a big believer in simplicity, right? So, I have used this as an example. If you think about it right, all the gridlines and the yellow colors are all what I call non-data ink, correct. It does not communicate one mile of additional information for me. Whereas the title that I put there called totals by specialty is correct that actually communicates something to me, yes yes right. So, to me that is data inc it is communicating something to me correct the numbers are communicating something to me right. So, any amount of ink that you use to communicate something is useful right um anything else is styling. Now styling there are times when you have you know I you know it is not about oh there should be no styling understand this even the table that I have on the right there is some amount of styling that is being done, yeah. But even that styling is also very specific for a purpose again going back to purpose. So, if you go back and say what is the purpose of putting those cells in a yellow background, yeah there is nothing. Then so again it tells back to the same thing what is the purpose of that. So, if I go back and say what is the purpose of the title it has a very clear purpose it tells me what this means is what uh. So, again going back see and you know also if you look at it that is the difference between purpose and message, yeah, correct correct correct. So, the purpose is it just tells me that it is caused by speciality. The message might be that PCP and PT dermatologists are the highest, yeah yeah correct correct correct. So, the difference between a purpose and devices. So, anything that you do any color ink that you use that does not serve a purpose and communicate information to the end user is non-data ink, right and again you cannot eliminate it. So, you want to one call the lines that we have the horizontal and the vertical lines to be non-data right. So, that is why I said; minimize your non-data ink and maximize your data ink and you cannot eliminate it correctly. And the last point about annotating the data is essentially to help the users is that correct yes. So, again right, very simple, just look at the two graphs. What the left graph does is it gives you an axis and then you know somebody has got to hold a finger or a ruler across this to measure and read off what those bars are, yeah yeah. It is simpler to get rid of the access tick marks but annotate the data. Now it is much easier just visually just looks so much easier to read, correct, correct. So, much easier on the right hand side, yes. So, uh. So, annotate the data right now and people will see this as the graphs get more and more crowded and you have more and more data points. Annotation tends to become a bit sticky because it starts looking very messy, right, yeah yeah. So, fair enough, fair enough right. So, again you go back and annotate critical points you do not have to annotate every single data point okay things like that. So, you know, let us say you have 300 points on the x-axis and then you know, can you imagine a graph which has 300 little labels like that? It just becomes noise. Right for annotation it is important but it is judicious and selective on the annotation, precisely I mean yeah. Simply because it is listed as one of the important principles, do not blindly use it, it has been our consistent message, yes, correct okay. So, these are four principles and my submission is that if you follow these principles right. Let us put it this way you will not be wrong in what you're presenting, right, yeah. So, I would say this as an actual put it and necessary but not sufficient condition for optimal representation of data, yeah, correct, right right yeah. In words that I will appeal to the audience of this class right these principles are necessary but not sufficient, correct correct correct correct. Now let us focus on the process of designing the visualization techniques and tools right. So, you have a three-step process: first of all trying to understand the message, then choosing a particular form and then designing that particular tool right. So, so again this here also when you say define the message what are you trying to communicate right and again there is a purpose what is the purpose of this thing. So, if you are looking at it as a single visual component right or a sub element of a graph then you can think in terms of a purpose or it could be or what is the message I am trying to communicate with this chart correct uh. So, there is a bit of a back and forth between a message and a purpose, the distinction I made but the concept is the same right um what is the message right. And how do I ensure that when the person reading sees the graph he or she gets the same message that I am trying to communicate, yes that is very important. That is the key right that yeah what is being transmitted has to be received it has to be what is received right no loss in translation. Yes no loss in translation checksum should be there yes uh. So, that is fun and that is the key right. So, if I am trying to say something to somebody because you are not always going to be standing around trying to gain an ability to explain that graph. Somebody is going to pull up that graph or chart or whatever it is without the benefit of you having standing next to them, yeah correct. Which means that it is your responsibility to ensure that anybody or random let us say somebody has printed that and you see that graph when you buy peanuts on the beach on the paper that is that when you open it up you should be able to understand what the graph is, exactly, yeah yeah yeah. So, that is important right, what am I trying to communicate? How do I make that message clear? And at a glance you cannot come back and say listen if you spend 25 minutes looking through the graph and looking at the axis and this and that and this and you will also get the same message, yeah. You should not have to use the magnifying glass to decipher the message exactly correctly. So, no fine print right yes. Then I always talk about sometimes the best way to communicate is just write a paragraph, yeah yeah. If that is the best way to get the message across please do so. There is nothing that says that you have to show it graphically, correct, yeah. Sometimes you know I use diagrams like a flow diagram or a you know workflow diagram and you know this is you know especially in factories you will always know how material flows and you know um or sometimes it would be a graph sometimes it could be a tabular display. So, you have to choose the form of the visual display that is best aligned to the message you are trying to communicate, yeah yeah. That linkage is important: you have a message and what is the visual representative form that unambiguously conveys the same message that I want to convey, yeah that is critical, correct. And then there is a set of design principles correct. And these design principles come into play to ensure that visual cognitive cognition is correct. Yes, do not confuse you and you know I will spend a lot of time on that as well because I think that is where people also go terribly wrong at times. Choosing the right form in my opinion is actually much easier because once you get a clear message the form almost suggests itself, yeah yeah yeah. But the design principle is what makes a difference. So, I am equal when I say that right I mean you know I am just trying to draw parallel to see what people might understand. I would go to the equivalent or you drive it through the offside because it is a nice half volley outside the half stump. The short that you play is natural; you do not try to hook that ball. You can drive it through half the side. Because it is a nice half volley through the half side so often marked once you get the message the form suggests itself naturally, correct, yes. Unless you are MS Dhoni you do not try to helicopter shot the ball over midwicket correctly if it is if the ball is outside the off stream you will hit it through the half side through the offside by and large it suggests itself, right uh. So, that is what we may say you know when you get the right message the form kind of suggests itself. But the design principles make a difference right and that is the difference between Rahul Marathe or Anand driving the ball through the covers and Virat Kohli driving the ball through the covers, yeah right yeah yeah and that is where the design principles come, right. People will pay money to see Virat Kohli do this and you know you and I will. I do not think people will watch it even if we pay money to do it. So, I think that is where the difference comes correct. So, we will just go through a few of those things exactly. So, let us focus on these three things a little bit more right, yeah okay. I think we have a few examples that we will talk about right precisely precisely, yes. This is okay this is where we can actually end this session and move on to the next session.",
      "duration_minutes": 45,
      "keywords": [
        "visualization",
        "data integrity",
        "data ink",
        "annotation",
        "purpose",
        "communication",
        "cognitive processes",
        "decision-making",
        "visual representation"
      ]
    },
    {
      "lecture_id": 2,
      "week_id": 1,
      "order": 2,
      "title": "Visualization Types and Applications",
      "resource_type": "pdf",
      "resource_url": "BA 1.pdf",
      "content_extract": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools make it easier to see and understand trends, outliers, and patterns in data. It is a powerful way to communicate complex information concisely and effectively. Good decisions are based on an accurate understanding of good data. This involves not just having access to data but being able to interpret it accurately and present it clearly. Visualizing data helps decision-makers grasp difficult concepts, identify new patterns, and communicate insights more effectively. Vision is our most powerful sense, with approximately 70% of our sensory receptors located in our eyes, making visual representation of information an efficient and impactful method of communication. Data can be classified into different types based on its nature and the method of measurement. For example, categorical data represents defined groups or categories, such as customer ratings or city names. Counted data refers to discrete quantities like the number of cars owned or defects per hour. Measured data includes continuous variables like time or pressure. Each type of data requires a specific form of visualization to represent it accurately. Effective data visualization has several benefits. It allows the communication of complex information in a concise and powerful way. By creating a visual 'picture,' it becomes easier to reason about and analyze quantitative and conceptual information. Visuals make cognitive processing more efficient and provide a content-rich view at a glance. Furthermore, effective visual representations direct the viewer's attention toward the content rather than the methodology used to create the visualization. They also help to describe, explore, and summarize a dataset while conveying the significance of the data. There are three core attributes of visual perception that must be considered when designing data visualizations: form, color, and spatial positioning. Each of these elements plays a crucial role in how data is interpreted and understood. For instance, form refers to the shape and structure of the visualization, color can be used to highlight key areas or distinguish between categories, and spatial positioning determines how data points are arranged on a visual plane. There are four key principles that guide effective data visualization: ensuring integrity, knowing the purpose, maximizing data ink while minimizing non-data ink, and showing the data while providing annotations. These principles help ensure that visualizations are accurate, purposeful, clear, and informative. Ensuring integrity means that the data should not only be factually correct but also presented in a way that does not distort the truth. A classic example of distortion is manipulating the scales of a graph to exaggerate or downplay trends. This can mislead the audience and result in poor decision-making. Therefore, it is essential to use consistent scales and honest representations of the data. Knowing the purpose is fundamental to effective data visualization. Every table or graph should be designed with a clear purpose in mind. For example, if the goal is to show that only a small percentage of a patient base qualifies for a therapeutic regimen, the visualization should clearly and effectively convey that message. Importantly, a purpose is not the same as a message – the purpose guides the design, while the message is the takeaway insight. Maximizing data ink and minimizing non-data ink refers to using the least amount of visual elements necessary to convey the most amount of information. Redundant or decorative elements should be removed to prevent clutter. This concept aligns with Edward Tufte’s principle of maximizing the data-ink ratio, which emphasizes clarity and efficiency in visual design. For example, avoid 3D effects and unnecessary gridlines, and instead use subtle but clear markers to indicate data points. Showing the data and using annotations enhances the interpretability of a visualization. Annotations highlight key insights and guide the audience's focus. For instance, annotating a chart to point out an important trend or data anomaly can provide context and prevent misinterpretation. The process of creating an effective data display involves three steps: defining the message, choosing the appropriate form, and designing the display. Defining the message requires identifying the key takeaway from the data. For instance, if you are presenting data about AIDS cases in the U.S., the message may be that the majority of cases occur within the 25-44 age group, with a particular concentration in the 35-39 age bracket. Choosing the appropriate form involves selecting the best visualization type to convey the message. Different types of data and messages require different forms of visualization. If the message emphasizes the components of one item, a pie chart may be suitable. For comparing multiple items, a bar chart is more appropriate. When showing change over time, a line or column chart is typically best. If the goal is to display frequency or distribution, a histogram works well. For correlation analysis, paired bar charts or scatter plots are ideal. The final step is designing the display itself. This involves applying best practices to create a clear and effective visualization. For instance, avoid using 3D effects, as they can distort perception. Instead of using legends, consider placing labels directly on the data. Minimize visual clutter by using thin lines, thin axes, and minimal tick marks. Axes should always be labeled clearly, and where possible, value labels should be used directly on the chart. Dashboards are a specific application of data visualization that present key information on a single screen. A dashboard is defined as a visual display of the most important information needed to achieve one or more objectives, consolidated on a single screen for quick monitoring and understanding. When designing dashboards, it is crucial to prioritize the most essential data and present it in a clear and concise manner. Domain-specific knowledge may dictate what should be included on a dashboard, but general best practices still apply. For example, it is often best to include descriptive statistics and to ensure that the dashboard is easily interpretable at a glance. Basic dashboard design principles include minimizing non-data ink, using clear labels, and focusing on the most important metrics. The goal is to provide a comprehensive yet uncluttered view that allows users to make informed decisions quickly. In conclusion, effective data visualization relies on clear principles and best practices to ensure that complex information is communicated accurately and efficiently. By ensuring data integrity, knowing the purpose, maximizing the data-ink ratio, and using appropriate annotations, data visualizations can provide powerful insights that drive better decision-making. Whether you are creating a simple chart or a comprehensive dashboard, these principles remain fundamental to presenting data in a meaningful and impactful way.",
      "duration_minutes": 75,
      "keywords": ["data visualization", "chart selection", "dashboard design", "visual perception", "data integrity"]
    },
    {
      "lecture_id": 3,
      "week_id": 2,
      "order": 1,
      "title": "Understanding Probability Distributions",
      "resource_type": "youtube",
      "video_url": "https://www.youtube.com/watch?v=yMFsKaMRqdw",
      "content_transcript": "Hi this is the second session of the business analytics course and we are going to discuss probability distributions. Most importantly we are going to discuss how are we going to fit a distribution to a given data. So, first of all let us do a recap, what are probability distributions? We already have discussed it in other courses but what do we think what do we recall as probability distributions. So, essentially probability distributions are some kind of a statistical model that shows possible outcomes of a particular event or course of action that event may take. So, essentially probability distributions for a discrete random variable may look like all the possible values of the random variable along with the corresponding probabilities that the random variable will take on that particular value. And for a continuous distributions we generally represent that by a density function. For example if you recall we may have said that the x axis represents the values of the random variable the y axis represents the probability and for a discrete random variable we will say that what is the probability that x takes on a value equal to one and then we would have said some probability what is the probability that x takes on a particular value 2 and we would have said some probability. So, this is how the probability distribution looks like for a discrete random variable. Now for a continuous random variable we still have the same format which essentially means that x axis still represents the value of the random variable y axis represents some form of probability but we do not say probability we usually if you recall we say density function. Density function and then we would have drawn something like this for potential values of the random variable x. What is the difference here in the earlier diagram we had discrete probability masses because the random variable was discrete. Here we have continuous values of the random variable and therefore we can't really say that there is a probability mass sitting at a particular point. For example let us say that we are still talking about x taking on a value equal to 2. We cannot say that this is the probability, this is only the probability density. So, we only talk about density and for density we need a small interval to actually define some probability. So, you recall all of that. So, the focus of this session is not to re-describe density functions and probability distributions. The focus of this session is to go one step beyond and say that well I have data now and how do I fit some distributions to data or what do I do with that data. So, for example let us say that we in academic settings we hear this quite a lot. So, grades of a course follow a normal distribution what do I mean by that. So, what do I mean by that essentially grades. So, a random variable is grades here. So, the grades out of 100. Let us say so, random variable is grades and then it follows a normal distribution which essentially means that we are going to follow assume that this is a nice well-shaped curve and then some people are going to get a very high mark some people are going to get very low marks unfortunately and there are whole bunch of people who are going to be in between. So, that is what we mean by normal distribution once again the y axis represents the density. So, this is just a recall or sometimes in the business settings we may say something like this: sales next month are expected to be uniformly distributed. So, what do we mean by that. So, I may say that sales can be as low as a hundred thousand dollars, sales can be as high as two hundred thousand dollars this is sales next month, sales in the next month So, it can be hundred thousand dollars or it can be two hundred thousand dollars but instead of assuming a normal distribution. So, on x axis is sales here is your 100000, here is your 200000 and we are saying that it is uniformly distributed. So, you know what uniform distribution is once again y axis represents the density. So, these are essentially probability distributions normal distribution uniform distribution. We have taken two examples of continuous distribution but you get the idea. So, that's how we define probability distributions that's how we use probability distributions. So, now how are we going to go about using data. So, let us say that I have business data that I have collected, the business data may be about sales volumes the business data may be about the defaulters on loans or the business data may be the salary hikes that the employees got in a particular year. It may be about any business context for this kind of a data we can directly use the data and use it in our simulations there is no need to fit any distributions. This is typically called trace driven simulation. So, let us say that we have collected sales volume over a period of time. Let us say we have a monthly sales volume for the last three years which essentially means that I have 36 values in my data-set. So, instead of first fitting a distribution to the 36 values and then using the distribution in my further analysis I can directly use these 36 values in my analysis. So, if I want to simulate I will simulate directly using these 36 values, this is generally called trace driven simulation. The second method is to actually fit a theoretical distribution. What do you mean by theoretical distribution, theoretical distribution is all these things that we spoke about earlier normal distribution, uniform distribution, binomial distribution for discrete, Poisson distribution for discrete, exponential distribution for continuous these are all theoretical distributions. So, what we may do is for the sales volume data that we may have, sales volume data that I may have i may try to fit, quote unquote fit a distribution to my data. And obviously I cannot simply say OK normal distribution fits very well I have to go beyond that and I have to actually check whether the fit that I have assumed is actually good. And I am using these terms in a very deliberate way because these are precisely the technical terms which are going to be helpful later on. So, we always are going to say we are going to fit a distribution we are going to check how good is this fitment. Now let us say that our business data that we have collected is a particularly tricky data set and it does not fit very well with lot of theoretical distributions or the other way around, theoretical, most of the theoretical distributions do not fit to our data. What are we going to do? Well it is not the end of the world instead of trying to fit already available distributions like a negative binomial or a double exponential. Instead of fitting those kind of already available distributions to the data what you can do is we can actually create our own distributions. I mean this is like making rules as we go along typically Kelvin category but we create our own distributions and those distributions are called empirical distributions. So, the sales volume data that I already spoke about using that data we say that well what would be the distribution where these 36 values could have come from. So, using these 36 values we build our own empirical distribution and use that distribution in our future analysis. Now what are these empirical distributions have you discussed empirical distributions in your earlier courses most probably you have. So, let us quickly recall that. So, what are these empirical distributions? Empirical distributions are essentially distributions built from the data that we already have collected. We are not fitting a distribution to the data we are actually building a distribution from the data that we have collected please notice the difference. So, let us go beyond. So, how does one build a distribution? First of all what are the building blocks when we say we are building a distribution. How do we build a distribution for example normal distribution let us take simplest, normal. If we were to say that I want to characterize a normal distribution what would we need to characterize a normal distribution well we will need the building blocks. So, what are these building blocks? So, essential building blocks of any distribution are the density functions, the distribution functions, and we may also want to define some moments, the first moment around the mean, the second moment around the mean which can be built using density also. So, we have to estimate these parameters. So, essentially defining a distribution means identifying a density function or a distribution function from the density function you can identify the building blocks like moments around the mean. Mean standard deviation and so on. Let us take an example of how to build a empirical distribution. So, let us say the data is ungrouped. So, let us say that we have collected X1 X2 X3 values. So, the X1 value, X2 value, X3 value and let us say all the way to X36. These are our 36 sales volume data for 36 months in our data set. Now what we are going to do is we are going to arrange them in a ascending order. So, X1 value was the first value that was recorded which was the first month but what we are going to do now is we are going to arrange it in a ascending order where the smallest value is called X bracket 1, OK X bracket 1, second smallest value is called X bracket 2 and the largest value is called X bracket n in our case X bracket 36, may not be the sales volume in the 36th month it is actually the maximum possible sales volume that we have found in our data set. So, these are called rank order statistics let us not worry about rank order statistics. So, once we have arranged the data in an ascending order you can actually define a distribution function in this way. This is not our own creation. These are, these definitions are usually available in any standard statistics textbook, all right! So, this is one way. I mean by no means we are saying that this is the only way of defining a distribution function. Now once we get a distribution function we all know how to get a density function and from density function we know how to get moments around the mean. This is for ungroup data. So, this is for ungroup data. Now if the data were grouped meaning that I only know that in this interval I have ten values in the other interval I have some eight values in some other interval I have some five values if I have group data. So, let us say that intervals we define k intervals. So, we have intervals k such intervals and I know that in each interval I have some n1, n2, n3 values. So, in the first interval I have n1 values in the second interval I have n2 values third interval I have n3 values kth interval I have nk values and that gives me my total sample size of n. So, what we can do is we can create a piece wise linear function G using this definition where each G of aj is essentially proportion of the samples, proportion of the observations up to that point up to that interval. So, once again a very non unique way of defining a distribution function. Once again notice that this is a distribution function why do I know that this is a distribution function because the value lesser than the smallest value is 0 and the value beyond the highest value is 1 which is a typical definition of a distribution function which goes from zero to one. And once again our usual methods are going to kick in where we have a distribution function from there we get the density function and so on. So, these are examples of how we can build empirical distribution. Let us go back why are we saying that why did we build these empirical distributions in the first place. We are saying that we have data we have collected data that data may be for any context it may be sales for our marketing data it may be financial analysis data it may be stock price data. So, let us say that a technical analyst wants to analyze wants to invest in the stock market. Now what are technical analysts well figure out why do not you search for it and then we will describe it in the next sessions. So, technical analysts let us say that they want to invest and for their investment decisions they have collected stock prices for the last three months. Let us say that I have actually tick level data, tick level data means I get data not every hour of a trading day, I may get data every minute or every second. So, I have huge data sets I mean that data set will be huge. Now I want to decide whether the stock is going to move up or move down. Now I have to predict whether I have whole massive data set of all the stock prices up to that point for the last three months and now I am saying tomorrow the market opens at 10 o'clock what is going to be the opening price of this particular stock for which I have collected data. Now how are you go about doing this we said the first option is to just use the three months of data that you have collected plain data that you have collected use the same values. That would be called trace driven simulation. Second approach would be for the three month data that you have collected why do not you fit a distribution and there has been enough and more research on what is a good fit for a stock price data. Obviously everybody wants to crack that problem and very clearly that I have not solved that problem because if I had cracked that problem I would not be sitting here it is already 11 o'clock I would be using my distribution and playing with the market. So, you can fit a distribution for the three months of data that you have collected and I have a whole bunch of candidate distributions available. Normal distribution, uniform distribution, log normal distribution, weibull distribution, the full family, not the full family, the full forest. And the third way is well the three months of data that I have collected is for a fairly weird stock, none of the distributions amicably fit the data and therefore I want to define my own distributions. And therefore we got into the empirical distributions. Therefore we got into the empirical distributions. So, these are two examples of how to build empirical distributions from the data that we have. Now let us go back and go to step number two what if I want to fit theoretical distributions how do I go about doing that. So, before we do that let us quickly take a look at how these three approaches compare with each other. Usually approach one which is using the plane three months data is usually used to validate the models. We already have a model you already have the output and you want to validate whether that output is correct or not. So, what you do is you push these three months of data into your model and your model generates an output and you compare that output with the reality with the existing system which is what happens tomorrow check and whether that matches. So, essentially our trace driven simulation is mainly used to fit to validate a model that you already may have built using something, some different approach. So, you have some prior knowledge how to build models for stock prices you have already done that now you want to check whether that model is correct or not. And therefore you feed into that model these three months of data whatever comes out of this model should match with what happened in reality or so should come close to each other. The drawback of this approach is you are going to test your model only with the data that you have collected. So, for example going back to the sales volume data you only have 36 values. So, your model is going to be tested only using the 36 values that you have actually observed and fed into the model that may not be enough that may not be enough. Even with the three months of minute level data on the stock prices let us say the stock price was fairly stable during these three months there was no turbulence in the market. So, how will you test whether your model works very well in the turbulent period. Now this data that you have collected will not give you that simulation because this data was collected from a fairly stable stock period, a stock market period. So, those are some of the problems. Approaches 2 and 3 building your distributions or using a theoretical distribution kind of avoid this problems. Because what you can do is once you have built a distribution you can generate values from those distributions which are not restricted to the 36 values that you have actually observed in your sample. So, compared to approach 1 I would say approach 2 and 3 are preferable that way. However if you can actually find a theoretical distribution that fits your data I would generally avoid building empirical distributions. Therefore I would say that theoretical distributions are preferred over empirical distributions. The problem with empirical distributions is very similar to the problem that we have for approach one. Now when you build an empirical distribution from the data that you have the distribution, the shape of the distribution is completely governed by the data that you have used to build the distribution functions. Remember your distribution functions, your distribution functions are built from the data that you have. So, the shape of the distribution will be completely governed by your data. Now once again if the data is of a particular pattern then quite likely that the distribution will be biased towards that. The other problem is the distribution that we built usually are restricted by the smallest and the largest value. So, here the distribution is 0 for all the values lesser than the smallest value that you have observed. The distribution is 1 beyond or the maximum value that you have observed in the sample which may not be true. This is the smallest value that I have observed in the sample does not mean that sales cannot be lower than this. This is the maximum value that I have observed in the sample does not mean that my sales cannot be more than that. However, the distribution that you build using these data will pretty much say so. The distribution that we have built will say that probability of finding a sales volume lesser than the smallest value is zero. And indirectly speaking probability of finding a sales value sales volume bigger than the maximum value is again 0 almost 0. So, those are the problems. So, we are still not able to go beyond whatever we have observed in our sample. So, that's, those are the problems. So, if you want to test the validity of our system from an empirical, from a data that comes from an empirical distribution we may have problem because we cannot simulate values which are outside of the range that was fed into. So, those are some of the issues with empirical distributions. Now there may be some compelling reasons for using a particular theoretical distributions. For example let us say that you have data about reliability. Now reliability engineering has a very high importance for weibull distribution. So, for any data that comes about distribution or the reliability I would actually I would like to test whether it fits the weibull family is it coming close there. So, those cases also I mean theoretical distributions why not test it before? So, those are the, that's the difference between fitting a theoretical distribution and fitting an empirical distribution.",
      "duration_minutes": 50,
      "keywords": ["probability distributions", "normal", "exponential", "uniform", "empirical", "density function", "distribution fitting"]
    },
    {
      "lecture_id": 4,
      "week_id": 2,
      "order": 2,
      "title": "Distribution Fitting Techniques",
      "resource_type": "pdf",
      "resource_url": "BA 2.pdf",
      "content_extract": "Probability distributions are fundamental statistical models that represent the possible outcomes of a particular event and their associated likelihoods. They help us understand how data is spread and what patterns emerge from real-world observations. For example, when we say that 'grades in a course follow a normal distribution,' it implies that most students' grades cluster around the mean, with fewer students achieving very high or very low scores. Similarly, saying 'sales for the next month may be uniformly distributed' suggests that all sales outcomes within a given range are equally likely. Identifying the correct probability distribution is critical in various business applications, such as predicting future sales, modeling loan defaults, or estimating salary hikes. The goal is to represent the data accurately and use this representation to simulate real-world scenarios. There are three primary ways to utilize collected data: (1) using raw data in simulations (trace-driven simulation), (2) fitting a theoretical distribution to the data, and (3) defining an empirical distribution directly from the data. Trace-driven simulation is useful but limited to historical observations. Theoretical and empirical distributions offer more flexibility, allowing us to simulate outcomes beyond observed data points.  Empirical distributions are constructed directly from the collected data. For ungrouped data, the empirical distribution function is built by arranging the data in ascending order and defining a piecewise function that maps each value to its cumulative frequency. For grouped data, the data points are binned into intervals, and a piecewise linear function approximates the cumulative distribution. This approach is simple but can exhibit irregularities if the sample size is small. Despite this, empirical distributions are useful when no well-known theoretical distribution fits the data accurately.  When comparing empirical and theoretical distributions, theoretical distributions are generally preferred if they provide a good fit. This preference stems from their smooth nature and the ability to generalize beyond the observed data. Empirical distributions, by contrast, are limited to the range of observed values and cannot extrapolate beyond them. In cases where physical or logical reasoning suggests a particular theoretical distribution, empirical data should still be used to validate the choice.  A real-world example involves fitting a distribution to 217 business-related data points. To begin, we examine the summary statistics. These statistics, including the mean, median, standard deviation, and coefficient of variation, offer initial clues about the distribution’s shape. For symmetric distributions, the mean and median should be approximately equal. If they diverge, the distribution may be skewed. For instance, if the coefficient of variation (CV) exceeds 1, the distribution is likely right-skewed and may be modeled by a log-normal distribution. In discrete distributions, the Lexis ratio serves a similar purpose to the CV in continuous cases. Another crucial measure is skewness (ν), which quantifies the asymmetry of a distribution. A normal distribution has a skewness of zero. Positive skewness indicates a rightward tail (e.g., an exponential distribution), while negative skewness suggests a leftward tail.  Once we hypothesize a distribution, the next step is parameter estimation. Each distribution is characterized by specific parameters—for example, the normal distribution by its mean and standard deviation, and the exponential distribution by its rate parameter (λ). The most common method for parameter estimation is Maximum Likelihood Estimation (MLE), which identifies the parameters that maximize the likelihood of observing the given data.  After estimating the parameters, we must assess the goodness-of-fit to determine how well the chosen distribution represents the data. Several methods are available for this purpose, including frequency comparisons, probability plots, and statistical tests. Probability plots provide a visual assessment. The quantile-quantile (Q-Q) plot compares the quantiles of the sample distribution against those of the fitted model. If the model is appropriate, the points should align along a straight line with an intercept of zero and a slope of one. Deviations from this line highlight discrepancies, particularly in the distribution’s tails. Another visual tool is the probability-probability (P-P) plot, which compares cumulative probabilities. While the Q-Q plot emphasizes differences in the tails, the P-P plot focuses on the distribution’s central portion.  Goodness-of-fit tests offer a formal statistical framework to evaluate the fit. The null hypothesis (H0) assumes that the data follow the specified distribution. Two widely used tests are the chi-square test and the Kolmogorov-Smirnov (K-S) test. The chi-square test divides the data range into intervals and compares the observed frequencies with the expected frequencies under the fitted distribution. The test statistic is calculated by summing the squared differences between observed and expected frequencies, normalized by the expected frequencies. This statistic is then compared with the chi-square distribution’s critical value, accounting for the degrees of freedom and the significance level (α). A significant result suggests that the fitted distribution does not adequately represent the data.  The Kolmogorov-Smirnov test, on the other hand, focuses on the maximum difference between the empirical and theoretical cumulative distribution functions. It is especially useful for continuous distributions and does not require binning the data. Like the chi-square test, the K-S test has a critical value that determines whether the observed data significantly deviate from the fitted model.  Choosing between these methods depends on the nature of the data and the analytical goals. For large datasets, visual tools like probability plots provide a quick assessment, while statistical tests offer more rigor. The chi-square test is versatile and works with both continuous and discrete data, whereas the K-S test is more suited to continuous distributions.  In practice, identifying the correct distribution involves a combination of exploratory data analysis, parameter estimation, and goodness-of-fit evaluation. This multi-step process ensures that the chosen model accurately reflects the underlying data patterns and can be used reliably for predictions and simulations. By carefully fitting and validating probability distributions, businesses can make data-driven decisions, model uncertainty, and improve operational efficiency across various domains.",
      "duration_minutes": 90,
      "keywords": ["distribution fitting", "empirical methods", "theoretical models", "maximum likelihood estimation", "goodness-of-fit tests", "probability plots", "Kolmogorov-Smirnov", "chi-square"]
    },
    {
      "lecture_id": 5,
      "week_id": 3,
      "order": 1,
      "title": "Statistical Association Analysis",
      "resource_type": "youtube",
      "video_url": "https://www.youtube.com/watch?v=obVldwZBm0c&t=4s",
      "content_transcript": "Welcome to this session on drawing inferences about the association between two categorical variable. In the previous session we had seen how to quantify association between two categorical variables through an application of conditional probabilities. Let us extend that discussion and. Now focus on drawing inferences about the association between the variables. And right. Now we are limiting our discussion to association between two categorical variables. So, let us take an example let us take this example of brand preferences let us say that there are three brands and there is brand A brand B and brand C and let us say that there are different preferences among the brand A brand B and brand C in different cities. And this data let us say is collected from a survey that was conducted in two cities Mumbai and Chennai when we ask them what brand do you prefer? So, 279 people in Mumbai said that they prefer brand A. 73 people in Mumbai said they prefer brand B 225 people in Mumbai said that they prefer brand C. So, totally 577 respondents and this was the breakup of their brand preference. 403 people were surveyed in Chennai and their preference for brand A and B and C is in the second row all right. So, out of the 980 people who were surveyed 444 said that they prefer brand A 120 prefer preferred brand B and 416 preferred brand C. Now we saw analysis of this here you notice that there are two categorical variables which are the two categorical variables? There is one variable in the columns which is the brand Columns brand A brand B and brand C those are the columns. So, there is a column variable column variable is the brand and the row variable is the city row variable is the city. So, the city is we can call city as our exponentially variable and brand preference as our response variable. So, there are two variables and if you notice both of them are categorical Mumbai and Chennai are the categories of the variable called city. Brand A brand B and brand C are the categories of our variable brand or brand preference. So, now we want to infer about the association between the two exponent two categorical variable. So, we essentially know how to summarize this data by calculating what is marginal probability and joint probabilities do you recall that what was marginal probability? What was joint probability? We saw if you recall we said whatever is in the margins is called marginal probability. So, 577 divided by 980, 444 divided by 980 what does 577 divided by 980 means it is the probability of randomly selecting a respondent from Mumbai city. 444 divided by 980 is the probability of randomly selecting a person who prefers brand A. So, those were marginal probability. Now do you recall what was joint probabilities? You go back to that session and understand the definition of joint probabilities joint probabilities is somebody are respondent being from Mumbai and preferring brand A. Somebody who is from Chennai an she prefers brand B that will be the joint probability of definition. So, essentially we wanted to ask a question whether brand preference is associated with the city? If the brand preference was not associated with city the responses would have been similar right with responses would have been similar. So, we are asking a question, are these responses similar? Are these responses similar? So, we want to use statistical independence statistical dependence for this. So, So, from the conditional probability discussion you remember that we said the categorical variables are statistically independent. If the conditional distributions for them is identical for each category that is what I said here if the responses are similar to each other similar to each other which means that the conditional probabilities are similar what what what would that table look like? That table will look like something like this. So, if we had a third city called Delhi and we get something like this. So, we we surveyed thousand people in Mumbai we surveyed hundred people in Chennai we surveyed 250 people in Delhi and this was their brand preference but look at the rows 44% of Mumbai residents prefer brand 14% of Mumbai residents preferred brand B 42% preferred brand C in Mumbai. But the proportion was same in Chennai and Delhii. 44% from Chennai also preferred brand A, 14% from Chennai also preferred brand B 42% of Chennai also preferred brand C. So, this 44%, 14% and 42% was the same similarly for Delhi 110 out of 250. So, 44% preferred brand 35 out of 250 which is about 14% preferred brand B 105 out of 250 which is about 42% preferred brand C. So, the proportion did not change proportion did not change. So, in such a case we can say that the two categorical variables are independent two categorical variables are independent and we can conclude that really brand preference does not seem to be dependent on cities. You go to any cities 44% approximately 44% people prefer brand A Approximately 42% people brand prefer brand C and brand B gets the least preference out of these three brands. Irrespective of the city that you pick but this was this was still based on the data that we have collected this was still based on the sample of 1000 people sample of 100 people sample of 250 people in each one of the cities. So,I mean before we go there you also remember the discussion that we we said that the statistical independence is actually a symmetric property. So, if a brand preference is independent of city, city is also independent of brand preference. So, if you actually calculate the proportion for columns here we calculated the proportion for rows. So, 100% out of 100% respondents 44% prefer brand A 14% prefer brand B and 42% preferred brand C this was the row proportion. If you calculate the column proportion even that is going to be same right even that is going to be same. So, that is going to be the same that tells you that statistical independence is actually a symmetric property but as I said this is based on sample data what about the population? Can I generalize these results? can I generalize these results and say that this independence will actually apply to the entire population. From the sample of 1000+100+250 across the three cities we seem to think that brand preference is independent of the cities does the conclusion extend to the population? Which is essentially inferencing can I infer about the population from this sample in that case simply looking at conditional probabilities may not be sufficient we may have to look at something more. So, can we draw inferences about the population from this single sample from this single sample of 980 respondent this single sample of 980 respondents. I have collected a single sample 980 respondents 577 from Mumbai 403 from Chennai. Can I conclude about the entire population who prefer brand A or brand B or brand C in these two cities by looking at only this sample that is the that is the objective of this session. So, how are we going to do this? We are going to do this by testing hypothesis obviously that is what we have been doing for inferences. Remember from from your BDM course. How do we infer about the population? We infer about the population by running a hypothesis test. This is a special hypothesis test because it has a very different test statistics. So, what is the hypothesis? The hypothesis is that the categorical variables are independent it is always the no effect null hypothesis. Now null hypothesis is always the no effect null hypothesis. Now hypothesis is these two categorical variables the brand preference and the cities are independent no effect hypothesis. Alternative hypothesis is no no they are not independent they may be dependent for that what we are going to calculate observed frequencies and expected frequencies. Observed frequencies come from the sample expected frequencies are going to be calculated assuming that the null hypothesis is true. Assuming that the variables are independent what frequencies do I expect? What frequencies do I expect Now I am going to do this indirect validation of my null hypothesis or not by comparing the observed frequency with expected frequency. Now let us let us do this and calculate the test statistic. So, what is what is the observed frequency? Observed frequency directly comes from the sample these are the observed frequencies 279, 165, 73, 225 these are the observed frequency. 225 people really in Mumbai prefer brand C, 47 people in Chennai actually prefer brand B this is actual observation. So, this is observed frequency 165 is the observed frequency. 279 is the observed frequency. Now let us calculate the expected frequency. Let us calculate the expected frequency. How is the expected frequency going to be calculated? It is going to be calculated as the product of row total into column total. Row total into column total divided by the total sample size let us calculate this. So, how do you calculate the expected frequency? How do you calculate this 261.4? This 261.4 is calculated as row total which is 577 577 multiplied by column total which is 444 divided by 980 total sample size. You can verify that that turns out to be 261.4 how did we get this 70.7? This 70.7 was calculated as row total, row total was 577 multiplied by column total column total is 120 divided by 980. How did we calculate this 171? 171 was calculated as row total which is 403 multiplied by column total which is 416 divided by total sample size which is 980. This is how I calculate the expected frequency. This is a calculated this is how I calculated the expected frequency. 279 is the observed frequency 261 is the expected frequency. Now I calculate my test statistic which is called the chi squared test statistic. The chi- squared test statistic is calculated as summation of the observed frequency minus expected frequency squared divided by the expected frequency. So, let me let me do this let me do this just to show you 279 and 261.4 observed frequency and expected frequency how are we going to calculate the Chi squared for that cell. 279 minus 261.4 whole squared divided by 261.4 261.4. So, this is the first cell right observed frequency 279 minus expected frequency 261.4 square it divided by the expected frequency plus summation sign is plus the second cell what goes in the second cell? Second cell is 73 is the observed frequency 70.7 is the expected frequency 70.7 is the expected frequency squared divide that by the expected frequency plus the third cell. 225 is the observed frequency 224.9 224.9 is the expected frequency. So 225 minus 244.9 square this whole numerator divide that by 244.9 is the third entry. Similarly for this cell this cell and this cell. So, +165 is the observed frequency 182.6 is the expected frequency 182.6 is the expected frequency divide that by 182.6. And we do that for the remaining two cells and the summation of all this all of this is going to be called the Chi squared test statistic Chi squared test statistic. Now what is going to happen if this expected frequency is going to be very close to the observed frequency if the expected frequency is going to be very close to the observed frequency what will happen? we will get very small numerator we are going to square it we will square it essentially to remove the negative signs. If the expected frequency is very close to the observed frequency the numerator is going to be fairly small and therefore the value of the test statistic is going to be smaller. But when am I going to get expected frequency close to the observed frequency when am I going to get that when am I going to get that? I am going to get that So, I am going to get this observed frequency close to the expected frequency only when the null hypothesis actually is true only when the category will be able to be independent only when the categorical variables are independent. So, when the null hypothesis is actually true the expected frequencies and observed frequencies are going to become close to each other and the test statistic is going to be relatively small. However if the null hypothesis is actually not true then for at least some of the cells the gap between the expected frequency and the observed frequency will be quite large and therefore that will result in a very large value of the test statistics. So, how do I decide whether the hypothesis is true or not? Simply by looking at the test statistics. If the test statistic is larger in value that gives me evidence that the null hypothesis may not be true and therefore should be rejected and vice versa. So, essentially a large value of test statistic is actually evidence against the null hypothesis. It is actually an evidence against the null hypothesis. For Chi squared test statistic you actually need degrees of freedom degrees of freedom is actually calculated as row minus 1 multiplied by column minus 1. For example how many rows there are two cities. So, the number of rows is equal to 2, r equal to 2. In our example therefore r minus 1 will be one. How many columns there are three columns for three brands and therefore C is 3, C minus 1 will be 2 and therefore degree of freedom for our test will actually be one multiplied by two row r minus 1 will be 1, column C minus 1 will be 2. So, 1 multiplied by 2 is 2 there are 2 degrees of freedom for our Chi squared test and we are going to get our Chi square test from the table and we are going to calculate Chi square test calculate Chi squared value using this equation. Compare the two and decide whether my Chi square value is large or not. So, for our for our brand preference example it turns out that the calculated test statistic turns out to be 7. So, this this turns out to be 7 calculated value calculated value turns out to be 7. What is the tabular value for two degrees of freedom at 95% confidence our tabular value of test statistic turns out to be 5.99 clearly the calculated value is bigger than the tabular value. Therefore we reject the null hypothesis. We reject the null hypothesis and conclude what was the null hypothesis? That the null hypothesis was that the categorical variables are independent we reject this null hypothesis and therefore say that brand preference does depend on the cities. If you go to different cities people are going to have different brand preferences that is our conclusion at 95% confidence. That is our conclusion at 95% confident which means that if somebody wants to be 95 somebody wants me to be 95% confident I will say that cities do impact brand preference they are not independent and I am saying this with 90% confidence 95% confidence. What if somebody wants me to 99% confident. What if somebody wants me to be 99% confident. If I want to be 99% confidence the alpha value turns out to be 0.01 and the tabular value increases. Tabular value goes up from 5.99 to 9.21. Now comparing 9.21 with 7 suddenly the 7 does not seem to be large value. Since the calculated test statistic is smaller than the tabular value of the test statistic I end up not rejecting the null hypothesis. What do I mean by not rejecting null hypothesis. I cannot reject the null hypothesis and I will conclude that brand preference does not depend on cities. Earlier when somebody wanted me to be 95% confident I concluded that brand preference changes with cities. If somebody wants me to be 99% confident however my result changes now I end up saying that those two categorical variables are actually independent. So, it does not matter which city you go to brand preference remains almost same. Now when I am saying this when I am drawing these conclusions when concluding about the hypothesis I am essentially inferencing about the entire population and not only these 980 value that we have collected from the survey. So, now our results are more generalized. Earlier when we looked at the conditional probabilities we could say only about the sample we could say from the sample it appears. Remember that conditional probability example was about possibility of what was about the MBA admissions given to per male and female candidates. Now we are talking about the entire population and not only the 980 values that we have collected okay not only the 980 values that we have collected. So, that is we drawing inferences about the association between two categorical variables using a Chi squared test of independence. Let us end the session here.",
      "duration_minutes": 55,
      "keywords": ["contingency", "independence", "joint probability", "marginal", "chi-square", "hypothesis testing", "expected frequency"]
    },
    {
      "lecture_id": 6,
      "week_id": 3,
      "order": 2,
      "title": "Bayesian Analysis in Business",
      "resource_type": "pdf",
      "resource_url": "BA 3.pdf",
      "content_extract": "Determining and inferring association is a fundamental aspect of statistical analysis that involves understanding relationships between variables. Consider a scenario where a B-school shortlisted 1200 candidates (960 men and 240 women) for its post-graduate management program. Out of these candidates, 324 were offered admission. A women's forum raised concerns about gender discrimination as 288 men received offers compared to only 36 women. To analyze the situation statistically, we define key events: M represents the event that a candidate is male, F represents the event that a candidate is female, A represents the event of receiving an admission offer, and Ac represents the event of not receiving an admission offer (the complement of A, where Pr(A) + Pr(Ac) = 1). We calculate the joint probabilities of these events. For example, the probability that a randomly selected candidate is male and is offered admission is Pr(M∩A) = 288/1200 = 0.24. Similarly, the probability that a candidate is male but not offered admission is Pr(M∩Ac) = 672/1200 = 0.56. For female candidates, Pr(F∩A) = 36/1200 = 0.03 and Pr(F∩Ac) = 204/1200 = 0.17. These probabilities allow us to create a contingency table, showing both joint and marginal probabilities. Marginal probabilities represent the probability of a single event without consideration of the other (e.g., Pr(M) = 0.8 and Pr(F) = 0.2).  To further analyze the association, we calculate conditional probabilities. For instance, Pr(A|M) represents the probability of being offered admission given that the candidate is male. This is calculated as Pr(A|M) = Pr(M∩A) / Pr(M) = 0.24 / 0.8 = 0.3. For female candidates, Pr(A|F) = Pr(F∩A) / Pr(F) = 0.03 / 0.2 = 0.15. Thus, the probability of admission for male candidates is twice that for female candidates, suggesting a possible disparity, though this alone does not confirm discrimination. Bayes' rule is a crucial tool for updating prior probabilities based on new evidence. It allows us to compute the posterior probability of an event after considering new information. For example, suppose a manufacturer receives raw materials from two suppliers: 65% from Supplier S1 and 35% from Supplier S2. Historical data indicates that 98% of S1's materials are good quality, while 95% of S2's materials meet the same standard. Using Bayes' rule, we can compute the joint probabilities: Pr(S1∩G) = Pr(S1) * Pr(G|S1) = 0.65 * 0.98 = 0.637, and Pr(S2∩G) = 0.35 * 0.95 = 0.3325. If the manufacturer encounters a defective product, they can use Bayes' theorem to identify the most likely supplier responsible by calculating the posterior probabilities Pr(S1|B) and Pr(S2|B).  Association analysis extends to categorical data, where we often test whether two categorical variables are statistically independent. For instance, a survey conducted in Mumbai and Chennai examines preferences for three brands. The city is the independent variable, while brand preference is the dependent variable. By calculating joint and marginal probabilities, we investigate whether brand preference depends on location. If the conditional distributions are the same across categories, we conclude independence; otherwise, there is evidence of association. Extending the analysis to a third city allows us to check if the conditional distribution remains consistent. Statistical independence implies symmetry: if the rows are independent of the columns, the reverse must also hold.  To formally test for association, we use the chi-square (χ²) test of independence. The null hypothesis (H0) states that the variables are independent, while the alternative hypothesis (H1) suggests they are not. We compare observed frequencies (fo) with expected frequencies (fe), where expected frequency for each cell is calculated by multiplying the corresponding row and column totals and dividing by the sample size. The chi-square statistic is computed as the sum of the squared differences between observed and expected frequencies, normalized by the expected frequency.  In the brand preference example, the calculated chi-square value is χ² = 7.0 with 2 degrees of freedom (df = (r-1) * (c-1)). At a significance level of α = 0.05 (95% confidence), the critical chi-square value is 5.99. Since 7.0 exceeds 5.99, we reject the null hypothesis, indicating a significant association between city and brand preference. However, at a more stringent α = 0.01 (99% confidence), the critical value is 9.21, so we fail to reject the null hypothesis. This demonstrates how the strength of evidence depends on the chosen confidence level.  In summary, association analysis involves using contingency tables, conditional probabilities, and chi-square tests to examine relationships between variables. Bayes' rule provides a framework for updating beliefs based on new evidence. Understanding these statistical methods is essential for drawing valid inferences about relationships in data.",
      "duration_minutes": 75,
      "keywords": ["bayes", "prior", "posterior", "updating", "decision", "chi-square", "association", "conditional probability"]
    },
    {
      "lecture_id": 7,
      "week_id": 4,
      "order": 1,
      "title": "Demand Response Curve Fundamentals",
      "resource_type": "youtube",
      "video_url": "https://www.youtube.com/watch?v=vTl1tQklAQo&t=1s",
      "content_transcript": "Welcome to the new session. In this session, we are going  to focus on the relationship between price and demand through a curve called demand response  curve. So, what is the demand response curve? So, generally the retailers are going to offer a price  for a particular product or service in the market. Now, the market is going to react  by realizing a particular demand at that particular price point.  So, the demand response curve essentially tells us what is the realized demand at a price that is  offered for a particular product or service. Now, why is this a business analytics topic?  Well, the demand response curve itself is a to two variable plot price on the x axis  and demand on the y axis price on the x axis demand on the y axis. So at a particular point,  we keep changing the price and we keep realizing the demand at that particular price.  So, today's session or a couple of sessions are going to focus on how we estimate  this demand response curve. So, we are going to discuss various aspects of various  relationship types between price and demand. And in general, the basic mechanism of this  price and demand relationship through this demand response curve.  So, typically, this is what a demand response curve is going to look like.  So, we have this particular curve, a rough curve which is done like this, this is what we  are going to call the demand response curve. So, what does it say? So, at a particular price  let us say P1 this is the price offered and you hit the curve you look at this left and this  Q1 is the quantity demanded by the market at that particular price point.  Now, we know that we are going to adjust our demand based on the prices that are offered  in the market. For example, if the price is P3, the quantity that is going to be demanded by the  market is going to be Q3. So, every retailer has to decide what is the best price and let us call  this best price as P star what is the best price to be offered for that particular product or  service. At that particular optimal let us say optimal price point there is going to be some  optimal demand realized in the market. Now, let us say that we make a mistake in  calculating this optimal price and we end up pricing more. There is always a potential for the  retailer to reduce the price which means go left to reduce the price and capture more demand  because at P star the demand is going to be Q star, if we reduce the price if we reduce  the price from P star to P1, the demand is going to jump up from Q star to Q1.  So, this is that there is always a latent demand which is realized by this part of the region. This  region represents the latent demand. How do we capture the latent demand? By reducing the price,  by reducing the price we can capture more and more demand. Now, this portion of the plot represents  what is called consumer surplus. Now, consumer surplus essentially is the benefit that the  consumer gets by paying less and less price. So, if we are actually increasing the price if  we are actually increasing the price from P star to P3, let us say the consumer surplus is going  to be eaten away that much. So, this light blue shaded region is going to be the reduction in the  consumer surplus that the consumer is getting, because the price is at P star and not at P3.  If the price are if the prices are increased from P star to P3, the consumer surplus is  going to consumer surplus is going to go down by this much amount by the  light blue region. So, this is how the region is going to be  read, this is how the plot has to be read as I was saying. So, this optimal price P  star has to be very carefully chosen. So, if we reduce the price, there is always a scope to  capture more demand, if we increase the price the consumer surplus may get affected we have I mean  just like we are talking about consumer surplus we can talk about the producer surplus I  mean I am pointing you to the basic economics course which is the slightly outside the  current discussion on forecasting the curve, but nevertheless important for the discussion.  Another important question then is if this is the price rate, if this is the demand response curve,  how should the retailer decide this optimal price? What should be the objective of finalizing this  price? Now, there are a couple of ways that we can go about. One is what is called as revenue  maximizing price a revenue maximizing price every retailer wants to maximize the revenue the  money that they collect from the customers of the product and service. So, that  should always be a concern. However, we have to realize that maximizing  revenue may not be the same as maximizing profit. So, there is a profit maximizing price, the price at which the profit is maximized,  the price at which the revenue is maximized and we have to keep in mind that these two may be  different objectives. Sorry, these are different objectives. We have to keep in mind that the  optimal prices when we are maximizing revenue may be different from the optimal prices  when we are maximizing the profit. How do we go about with these two objectives? Let  us discuss that later. So, this is the basics of the demand response curve which is the  blue curve which is pointed here. Let us understand the properties of this.  So, this is essentially a function that describes how the demand varies as a function of price very  similar to the demand supply curve in economics, however, this is for a single seller in a single  market whereas, the demand supply curve aggregates various supply in the market and  aggregates various demands in the market. So, this is slightly different from that scenario,  where we are considering a single seller at a single time point in a single market.  If you notice, there are four important things that you notice from this demand response curve.  First of all, the demand for a product is always going to be non-negative, which means that it is  always going to lie on the positive side of the y axis. You cannot have demand going negative.  You cannot have demand going less than 0 does not make sense. Demand cannot be negative.  So, the demand response curve is always going to be on the positive side. Similarly, you cannot  offer negative prices, I mean, let us not get into the details, but negative prices would  mean that the retailer gives money to the customer for using the product and  services, which generally does not happen in the market. Therefore, even the prices  are going to be on the positive side. So, you do not expect prices to go on the negative  side. The lowest price is going to be 0. We are not going to look at a case where negative prices  are even possible. So, the curve is going to be non-negative in both senses. The demand is going  to be positive even if the prices offered in the market are going to be positive . It is going  to be a continuous curve. So this is going to be a nice smooth continuous curve without  any breakages without any breakages. So, for example, this is the price and this is  the demand. You do not expect something like this, which means that you do not know what the demand  is going to be between these two price points. So, what is going to be the demand? So, are we  saying that there is no demand, but that also means that demand is 0. So, there is not going  to be a discontinuity in the curve, there is not going to be a discontinuity in the curve.  Very similarly, the curve is going to be very smooth and differentiable which means that  tangent is always possible tangent is always possible at all the price points tangents is  always possible at all the price points. So, the curve is also going to be differentiable.  And unless we are talking about very specialized goods, that curve is generally going to be  downward sloping. What does it mean? It means that as the prices  increase the demand for the product reduces. There are certain products for  example you can always think of Giffen goods. Examples could be luxury items  for example, Rolex watches, Rolex watches sometimes if you increase the prices  if you increase the prices, the demand for the product may actually go up because the exclusivity  appeal for the product may go up. So, but generally speaking, we  are going to look at goods whereas the prices are increasing, the demand is actually  going to come down. So if the prices increase, the demand is going to come down and therefore,  the curve is going to be a downward sloping curve, downward sloping curve. So, these are  the four important properties once again, the quantity is going to be non-negative,  the prices are going to be non-negative, the prices are going to be non-negative.  So, it is first quadrant curve, the curve is coat and coat nice, which means that it is continuous  it is differentiable and more importantly, we are going to look at a scenario where the curve  is downward sloping the curve is downward sloping, which means that as the prices go up, the  demand goes down, the demand goes down. So, these four these four properties are  important. As I said, sometimes some goods may or may not hold but we are right now going  to not worry about those kinds of goods. Right we are going to look at goods where the demand  response curve is a downward sloping curve. [Music] so two ways of looking at how how we measure the price sensitivity uh how how sensitive is the is the demand to the price right so two ways of looking at it one is simply one is simple slope which is how much the demand changes with response to the change in price uh so it is essentially the change in demand demand at uh price point p2 minus demand at a price point p1 minus p ah divided by p2 minus p1 so it's a it's so essentially the delta in the demand divided by delta in the price right so very very uh simple uh uh ratio of change in uh uh change in demand divided by change in price right so as we know since the curve is downward sloping downward sloping if p1 is greater than p2 this means that the demand at p1 is lesser than demand at p2 what does that mean we go back right we go back let us go back so if so let us look at let us look at what is this point here this point here is demand at p3 and this point here is demand at p1 now we know that p3 is greater than p1 the price p3 is greater than price p1 and we can also see that demand at p3 demand at p3 is here which is q3 so this is q3 which is always lesser than demand at p1 which is q1 right so q1 is greater than q3 whereas p3 is greater than p1 so we know that right which means that this slope this slope if you define the slope it is going to be a negative negative value the slope is going to be negative value so we can we can we can look at slope as a local local estimator of change in demand for a very small change in price so very simple ah just calculate the calculate the slope of the slope of the tangent at that particular point and we will get the ah we will get the ah sensitivity ah for that particular price there is other way to calculate price sensitivity and that is called demand elasticity okay that is called demand elasticity so it is the ratio of percentage change in demand to the percentage change in price notice the difference notice the difference right notice the difference so essentially it is this is change in demand with change in price this is percentage change in demand with percentage change in price so you divide then the numerator by demand at p1 you divide the denominator by price p1 right so so essentially the numerator becomes a unitless quantity because you are dividing demand by demand so this the units cancel out the denominator becomes a unitless quantity because it is price divided by price so units cancel out so essentially unlike slope elasticity is unit less quantity it's a unit less quantity right uh so it is remember i let us define it again percentage change in demand to the percentage change in price right so percentage change in demand to the percentage change in price so for example elasticity of 2 would mean that a 10 percent reduction in price uh is essentially increases the demand by 20 percent right that that that's the meaning of this two so 20 percent increase so 10 percent reduction in price increases the demand by 20 right so we are we are talking percentages we are talking percentages in elasticity unlike slope where we are talking about change in demand to change in price elasticity is percentage change in demand to percentage change in price so that's the difference between elasticity and slope of the demand response curve now uh what do what what is the what is the general interpretation of elasticity there are certain goods which are supposed to be less elastic for example ah let us take the example of common salt that we use in our food now salt is required without salt the food is just not going to taste which means that even if the prices of salt go up i don't expect our consumption of salt to to change that much because salt is needed salt is essential quantity right without salt the the the food is not going to taste so i would expect the elasticity of something like salt to be much lesser right otherwise uh go to the other extreme ah think about uh think about a service like a holiday right a service like a holiday now uh uh holiday uh if the if the holiday is going to cost us too much there is a very high probability that we may change our plan right most of us may want to change our plan we may still go to our holiday but we may probably choose a different service ah reduce the number of days or do something but essentially react to that change in price so i would say that a holiday is a service where the elasticity is generally quite high right so here uh there is another thing that we have to look at is elasticity may also depend on time elasticity may also depend on time so there is a short term elasticity and there may be a long term elasticity so here here is here are few examples right so for example uh here are few examples of as i said salt for a product like salt for a product like salt uh in short term i need salt i just need salt right i mean there is absolutely no there is no alternative to that so for a for a product like salt uh the elasticity may be i mean yeah zero right uh i i just i just need salt that's it i just need salt uh for example on the other hand uh a two wheeler a two wheeler right uh a two wheeler i will say that elasticity is quite large because uh uh if the two-wheeler is going to cost me too much uh i will say ah let let me take a bus today let me take a bus today and not buy this two wheeler right so i i may i may i may look for alternate modes of transport at least in the short term at least in the short term i may i may look for alternates uh alternatives to buying a two wheeler so in that sense the the the short term elasticity of two wheeler may be much much higher than a short term elasticity for sure for salt now let us say that there is some emergency meeting that i have to attend ah there may be some emergency meeting that i have to attend and i just have to take a flight i mean there is no alternative i just have to take a flight i have to go attend a meeting i have to go attend to a personal thing uh i just need to travel so in the short term in case of emergency for example we may argue that even if the prices are higher ah if the if the demand requires i mean if the if the situation requires that i travel i will have to travel i will have to travel so pop the airline travel may have a very very uh low short-term elasticity uh movies right movies uh if the movie tickets are expensive i may postpone i may say let me watch tv at home today instead of buying a 500 rupee movie ticket or something like that let let me go to my friend's house right but i i may want to postpone that that purchase because i may want to find alternatives whereas for a salt there is actually no alternative right so in those cases i may expect a larger short-term elasticity but as i said elasticity may also have a time axis which means that the long term elasticity may be different for example for air travel now if i have emergency requirement i must reach there faster and therefore whatever is the price i may want to pay and catch that flight and reach my destination however in the long term in the long term if i have if i can plan and if the price tickets if the flight tickets are really expensive i may still want to find alternate mode of transportation and therefore in the long term airline travel may have a large value for elasticity so in the short term emergency cases i don't care if the price of the flight is uh too much i have to travel therefore i have to travel elasticity may be low in the long term if you allow me to plan my trip carefully ah if the if the price points are just not acceptable to me i may find alternate modes of transportation and therefore the elasticity goes up significantly in the long term for salt the elasticity may not change that much right because as i said salt is essential commodity salt is essential commodity so salt is essential commemoration it may go up little bit maybe i i will i will say that anyway eating up too much of salt is bad for my health i may cut down on the quantity of salt i eat however the the the margin that i have is is quite less therefore the elasticity may not change drastically as it would change for airline travel ah look at uh look at two wheeler on the other hand for a two wheeler uh today i go to the showroom and i say the prices of two wheeler is too much and therefore let today at least at least for a week time let me manage with alternate modes of transportation uh but in the long term if there is a demand for two wheeler there is a demand for two wheeler i mean really in the long term you really can't avoid so there are certain goods just wanted to show you that there are certain goods where the elasticity may come down over a period of time there are certain goods where the elasticity may go up drastically over a period of time a petrol is the example of later ah two wheeler as an example of former right so for movies movies uh uh i mean in general if the ticket prices are expensive i i may find alternate modes of alternate modes of transportation uh alternate modes of entertainment alternate modes of entertainment but uh i may still go for a movie in the long term right uh if you allow me to plan longer time maybe i'll buy subscription to one of the ott and i never have to go for uh movie theater right so the elasticity may go up drastically over a longer period of time right so that that's how elasticity changes ah depending on the goods right depending on the goods and depending on the time frame depending on the time frame right sometimes it changes drastically like 0.1 to 2.4 these are by the way examples we are not saying that airline travel has a elasticity of 2.4 in the long term right it may depend on it may depend on general consumer behavior right for some people airline travel may not be that elastic because even in long term uh i i i may not want to prefer alternate modes of transportation uh if i want to go for a movie i i once again 3.7 is not going to be elasticity for everyone in the long term right so these are just ah these are just representative numbers they are not elasticity values for everybody in the short term or for the long term but i hope i have conveyed the concept of elasticity depending on the product and services or depending on the time frame depending on the time access all right so just to recap we have looked at price sensitivity using two methods one is calculating the slope the other one is calculating the elasticity and we have interpreted both of them ",
      "duration_minutes": 48,
      "keywords": ["demand", "price", "elasticity", "slope", "consumer surplus", "revenue", "profit"]
    },
    {
      "lecture_id": 8,
      "week_id": 4,
      "order": 2,
      "title": "Advanced Demand Modeling Techniques",
      "resource_type": "pdf",
      "resource_url": "BA 4.pdf",
      "content_extract": "The demand response curve is a critical concept in economics and business analytics, describing how the demand for a product changes in response to variations in its price. This curve is a specific form of the general demand curve used in economics but applies to a single seller operating within a defined market. Understanding the demand response curve is essential for optimizing pricing strategies, forecasting sales, and maximizing revenue. The function  𝐷 ( 𝑝 )  represents the quantity demanded as a function of the price  𝑝 , and it typically exhibits four key properties: it is non-negative, continuous, differentiable, and downward sloping. These properties imply that demand decreases as price increases and that the relationship is smooth and mathematically tractable. The slope of the demand curve reflects price sensitivity, which measures how responsive consumers are to changes in price. This sensitivity is formally captured through the concept of elasticity, which quantifies the percentage change in quantity demanded resulting from a one percent change in price. Two primary models are used to describe the demand response curve: the linear response curve and the constant elasticity curve. The linear demand response curve assumes a straightforward relationship between price and demand, which can be modeled using simple linear regression (SLR). In this model, the intercept ( 𝐷 0  ) represents the baseline demand when the price is zero, and the slope ( 𝑚 ) indicates how much demand decreases for each unit increase in price. By conducting market experiments where different prices are tested and corresponding demand levels are observed, it becomes possible to estimate these parameters and assess whether the linear model accurately describes the observed data. However, not all demand relationships are linear. The constant elasticity model accounts for cases where the elasticity remains the same across all price levels, leading to a curved demand response. This model can be transformed into a linear form through appropriate mathematical transformations, enabling the application of regression techniques to estimate its parameters. The estimation problem involves analyzing empirical data from market experiments to quantify how demand responds to price changes. In these experiments, price serves as the explanatory variable, while demand is the dependent variable. Statistical techniques such as regression analysis provide a framework for estimating the slope of the demand curve and calculating elasticity. A simple linear regression model is particularly useful for estimating the parameters of the linear demand response curve. By fitting a line to the observed price and demand data, analysts can determine whether a linear relationship adequately captures the underlying dynamics. This process involves minimizing the difference between the observed demand values and the values predicted by the model. If the linear model fits well, the estimated slope and intercept provide actionable insights into consumer behavior. In contrast, the constant elasticity model requires a more sophisticated approach due to its non-linear nature. By applying logarithmic transformations, the model can be recast in a linear form, allowing for the use of standard regression techniques. This approach provides a flexible and accurate means of modeling demand responses that exhibit constant elasticity across different price points. Understanding the demand response curve has practical implications for businesses seeking to optimize pricing strategies. By quantifying how sensitive demand is to price changes, firms can make informed decisions about setting prices to maximize revenue or market share. For example, if demand is highly elastic, a small decrease in price could lead to a substantial increase in sales, boosting overall revenue. Conversely, if demand is inelastic, price increases may not significantly reduce sales volumes, allowing the firm to increase profits through higher prices. Elasticity itself is categorized into three main types: elastic, inelastic, and unitary. When the absolute value of elasticity is greater than one, demand is considered elastic, meaning consumers are highly responsive to price changes. If the absolute value is less than one, demand is inelastic, indicating that price changes have a relatively minor effect on demand. When the elasticity equals one, the relationship is unitary, implying that a percentage change in price results in an equivalent percentage change in demand. Estimating elasticity accurately is crucial for making strategic business decisions. For instance, in markets where demand is elastic, competitive pricing strategies and promotional discounts can drive significant sales growth. On the other hand, in markets characterized by inelastic demand, businesses may focus on premium pricing and value-added services to enhance profitability. The distinction between linear and constant elasticity models reflects the complexity of real-world demand patterns. While the linear model provides a straightforward approximation, it may not capture more nuanced relationships where elasticity varies across price levels. In such cases, the constant elasticity model offers a more accurate representation of how consumers respond to price changes. In practice, the choice between these models depends on the nature of the data and the specific analytical objectives. The estimation process typically involves collecting data from controlled experiments or historical sales records, applying appropriate statistical models, and interpreting the results in the context of business goals. Advanced techniques such as multiple regression can also be used to account for additional factors that influence demand, such as advertising expenditures, seasonality, and competitor actions. Furthermore, the demand response curve is instrumental in demand forecasting, enabling businesses to predict future sales based on projected price changes. Accurate forecasts allow firms to manage inventory effectively, plan production schedules, and allocate marketing resources efficiently. This predictive capability is particularly valuable in industries with volatile demand patterns or rapidly changing competitive landscapes. Another important application of demand response analysis is in price optimization. By understanding how demand varies with price, businesses can identify optimal price points that balance revenue and market share objectives. This process often involves simulating different pricing scenarios and evaluating their impact on key performance indicators. For example, a retailer might use demand response analysis to determine the optimal discount level for a seasonal promotion, maximizing sales while maintaining profit margins. Additionally, demand response curves are used in consumer segmentation and targeted marketing. By analyzing how different customer segments respond to price changes, firms can tailor pricing strategies to specific market segments. This approach enhances customer satisfaction and loyalty while improving overall profitability. For instance, price-sensitive customers may be targeted with special discounts and promotional offers, while premium segments may receive exclusive products and personalized services. In summary, the demand response curve is a foundational concept in business analytics, providing a quantitative framework for understanding how price influences demand. Through empirical analysis and statistical modeling, businesses can estimate demand elasticity, evaluate different pricing strategies, and make data-driven decisions to achieve their objectives. Both linear and constant elasticity models offer valuable insights, with the choice of model depending on the specific characteristics of the market and the available data. By leveraging these analytical tools, firms can enhance their competitive position, optimize resource allocation, and drive sustainable growth in dynamic and competitive environments.",
      "duration_minutes": 70,
      "keywords": ["regression", "elasticity", "log-log", "transformation", "experiment", "demand curve", "pricing strategy", "business analytics"]
    }
  ],
  
  "questions": [
    {
      "question_id": 3001,
      "content": "Which of the following is NOT one of the four umbrella principles of effective data visualization?",
      "type": "MCQ",
      "question_options": [
        "Ensure integrity",
        "Know purpose",
        "Use 3D effects",
        "Maximize data ink"
      ],
      "correct_answer": 2,
      "points": 5,
      "explanation": "The four principles are: ensure integrity, know purpose, maximize data ink, and show your data. 3D effects are discouraged in effective visualization.",
      "course_id": 4,
      "week_id": 1,
      "lecture_id": 1,
      "status": "active",
      "tags": ["visualization", "principles"]
    },
    {
      "question_id": 3002,
      "content": "Which of the following visualization types would be most appropriate for showing the distribution of ages in a population?",
      "type": "MCQ",
      "question_options": [
        "Pie chart",
        "Histogram",
        "Scatter plot",
        "Line graph"
      ],
      "correct_answer": 1,
      "points": 5,
      "explanation": "Histograms are specifically designed to show distributions of continuous data like age groups.",
      "course_id": 4,
      "week_id": 1,
      "lecture_id": 1,
      "status": "active",
      "tags": ["visualization", "distributions"]
    },
    {
      "question_id": 3003,
      "content": "In a contingency table showing gender vs. product preference, what does a joint probability represent?",
      "type": "MCQ",
      "question_options": [
        "Probability of being male",
        "Probability of preferring Product A given female",
        "Probability of being female AND preferring Product B",
        "Total probability of preferring Product C"
      ],
      "correct_answer": 2,
      "points": 5,
      "explanation": "Joint probability represents the probability of two events occurring together (conjunction).",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 5,
      "status": "active",
      "tags": ["probability", "joint probability"]
    },
    {
      "question_id": 3004,
      "content": "Calculate P(A|B) given P(A) = 0.4, P(B) = 0.5, and P(A∩B) = 0.2",
      "type": "NUMERIC",
      "question_options": [],
      "correct_answer": 0.4,
      "points": 8,
      "explanation": "P(A|B) = P(A∩B)/P(B) = 0.2/0.5 = 0.4",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 5,
      "status": "active",
      "tags": ["conditional probability"]
    },
    {
      "question_id": 3005,
      "content": "Which of the following are required conditions for applying Bayes' Rule? Select all that apply.",
      "type": "MSQ",
      "question_options": [
        "Known prior probabilities",
        "Mutually exclusive events",
        "Conditional probabilities",
        "Independent events",
        "Continuous variables"
      ],
      "correct_answer": [0, 1, 2],
      "points": 8,
      "explanation": "Bayes' Rule requires known priors, mutually exclusive events, and conditional probabilities. Independence is not required and it works with discrete or continuous variables.",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 6,
      "status": "active",
      "tags": ["bayes rule", "probability"]
    },
    {
      "question_id": 3006,
      "content": "A chi-square test for independence yields a p-value of 0.03. What conclusion can be drawn at α=0.05?",
      "type": "MCQ",
      "question_options": [
        "Variables are independent",
        "Variables are dependent",
        "Test is inconclusive",
        "Need more data"
      ],
      "correct_answer": 1,
      "points": 5,
      "explanation": "Since p-value (0.03) < α (0.05), we reject the null hypothesis of independence.",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 5,
      "status": "active",
      "tags": ["chi-square", "hypothesis testing"]
    },
    {
      "question_id": 3007,
      "content": "Calculate the degrees of freedom for a chi-square test on a 3×4 contingency table",
      "type": "NUMERIC",
      "question_options": [],
      "correct_answer": 6,
      "points": 8,
      "explanation": "df = (rows-1)*(columns-1) = (3-1)*(4-1) = 6",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 5,
      "status": "active",
      "tags": ["chi-square", "degrees of freedom"]
    },
    {
      "question_id": 3008,
      "content": "Which of the following statements about statistical independence are true? Select all that apply.",
      "type": "MSQ",
      "question_options": [
        "P(A|B) = P(A)",
        "P(A∩B) = P(A)*P(B)",
        "Independent variables cannot be correlated",
        "Causation implies independence",
        "Dependence implies causation"
      ],
      "correct_answer": [0, 1],
      "points": 8,
      "explanation": "Independence means P(A|B)=P(A) and P(A∩B)=P(A)*P(B). Independent variables can have zero correlation but not necessarily, and causation/dependence don't imply each other.",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 5,
      "status": "active",
      "tags": ["independence", "probability"]
    },
    {
      "question_id": 3009,
      "content": "In a survey of 200 people, 120 prefer Product A and 80 prefer Product B. What is the marginal probability of preferring Product A?",
      "type": "NUMERIC",
      "question_options": [],
      "correct_answer": 0.6,
      "points": 5,
      "explanation": "Marginal probability = 120/200 = 0.6",
      "course_id": 4,
      "week_id": 3,
      "lecture_id": 5,
      "status": "active",
      "tags": ["marginal probability"]
    },
    {
      "question_id": 3010,
      "content": "Which visualization would best show the relationship between two continuous variables?",
      "type": "MCQ",
      "question_options": [
        "Bar chart",
        "Scatter plot",
        "Pie chart",
        "Histogram"
      ],
      "correct_answer": 1,
      "points": 5,
      "explanation": "Scatter plots are specifically designed to show relationships between two continuous variables.",
      "course_id": 4,
      "week_id": 1,
      "lecture_id": 2,
      "status": "active",
      "tags": ["visualization", "relationships"]
    },
    {
      "question_id": 3011,
      "content": "A demand response curve has which of the following properties? Select all that apply.",
      "type": "MSQ",
      "question_options": [
        "Non-negative",
        "Continuous",
        "Upward sloping",
        "Differentiable",
        "Linear"
      ],
      "correct_answer": [0, 1, 3],
      "points": 10,
      "explanation": "Demand curves are non-negative, continuous, and differentiable. They are typically downward sloping and not necessarily linear.",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 7,
      "status": "active",
      "tags": ["demand curve", "properties"]
    },
    {
      "question_id": 3012,
      "content": "Calculate price elasticity when a 5% price increase leads to a 10% demand decrease",
      "type": "NUMERIC",
      "question_options": [],
      "correct_answer": 2,
      "points": 10,
      "explanation": "Elasticity = (%ΔQ)/(%ΔP) = -10%/5% = -2 (absolute value is 2)",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["elasticity", "demand"]
    },
    {
      "question_id": 3013,
      "content": "Which of the following distributions would be most appropriate for modeling customer arrival times?",
      "type": "MCQ",
      "question_options": [
        "Normal",
        "Exponential",
        "Uniform",
        "Binomial"
      ],
      "correct_answer": 1,
      "points": 8,
      "explanation": "Exponential distributions are commonly used to model arrival times in queuing theory.",
      "course_id": 4,
      "week_id": 2,
      "lecture_id": 3,
      "status": "active",
      "tags": ["distributions", "modeling"]
    },
    {
      "question_id": 3014,
      "content": "In hypothesis testing, what does a p-value represent?",
      "type": "MCQ",
      "question_options": [
        "Probability the null hypothesis is true",
        "Probability of observing the data if null is true",
        "Probability the alternative is true",
        "Strength of the effect"
      ],
      "correct_answer": 1,
      "points": 8,
      "explanation": "The p-value is the probability of observing the data (or more extreme) assuming the null hypothesis is true.",
      "course_id": 4,
      "week_id": 2,
      "lecture_id": 4,
      "status": "active",
      "tags": ["hypothesis testing", "p-value"]
    },
    {
      "question_id": 3015,
      "content": "Which of the following are steps in distribution fitting? Select all that apply.",
      "type": "MSQ",
      "question_options": [
        "Parameter estimation",
        "Goodness-of-fit testing",
        "Data collection",
        "Hypothesis formulation",
        "Visual inspection"
      ],
      "correct_answer": [0, 1, 2, 4],
      "points": 10,
      "explanation": "Distribution fitting involves data collection, visual inspection, parameter estimation, and goodness-of-fit testing.",
      "course_id": 4,
      "week_id": 2,
      "lecture_id": 4,
      "status": "active",
      "tags": ["distribution fitting", "steps"]
    },
    {
      "question_id": 3016,
      "content": "Calculate the slope of a linear demand curve where price decreases from $10 to $8 and demand increases from 100 to 150 units",
      "type": "NUMERIC",
      "question_options": [],
      "correct_answer": 25,
      "points": 10,
      "explanation": "Slope = ΔQ/ΔP = (150-100)/(8-10) = 50/-2 = -25 (absolute value is 25)",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 7,
      "status": "active",
      "tags": ["demand curve", "slope"]
    },
    {
      "question_id": 3017,
      "content": "Which goodness-of-fit test is most appropriate for continuous distributions?",
      "type": "MCQ",
      "question_options": [
        "Chi-square",
        "Kolmogorov-Smirnov",
        "t-test",
        "ANOVA"
      ],
      "correct_answer": 1,
      "points": 8,
      "explanation": "Kolmogorov-Smirnov is specifically designed for continuous distributions.",
      "course_id": 4,
      "week_id": 2,
      "lecture_id": 4,
      "status": "active",
      "tags": ["goodness-of-fit", "continuous"]
    },
    {
      "question_id": 3018,
      "content": "In a market experiment, prices were tested at $5, $10, and $15 with corresponding demands of 200, 150, and 100. What type of elasticity does this suggest?",
      "type": "MCQ",
      "question_options": [
        "Perfectly elastic",
        "Unit elastic",
        "Inelastic",
        "Elastic"
      ],
      "correct_answer": 3,
      "points": 8,
      "explanation": "A 50% price increase leads to a 50% demand decrease (from 200 to 100), suggesting unit elasticity (absolute value of 1).",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["elasticity", "market experiment"]
    },
    {
      "question_id": 3019,
      "content": "Which of the following are assumptions of simple linear regression? Select all that apply.",
      "type": "MSQ",
      "question_options": [
        "Linear relationship",
        "Independent errors",
        "Normally distributed errors",
        "Constant variance",
        "Categorical variables"
      ],
      "correct_answer": [0, 1, 2, 3],
      "points": 10,
      "explanation": "SLR assumes linearity, independence, normality, and homoscedasticity (constant variance) of errors.",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["regression", "assumptions"]
    },
    {
      "question_id": 3020,
      "content": "What is the main advantage of a constant elasticity model over a linear demand model?",
      "type": "MCQ",
      "question_options": [
        "Easier to estimate",
        "Elasticity remains constant across price range",
        "Always provides better fit",
        "Requires less data"
      ],
      "correct_answer": 1,
      "points": 10,
      "explanation": "The key advantage is that elasticity remains constant across the entire price range in a constant elasticity model.",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["elasticity", "demand models"]
    },
    {
      "question_id": 3021,
      "content": "Calculate R² for a regression model where SSE = 50 and SST = 200",
      "type": "NUMERIC",
      "question_options": [],
      "correct_answer": 0.75,
      "points": 10,
      "explanation": "R² = 1 - (SSE/SST) = 1 - (50/200) = 0.75",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["regression", "r-squared"]
    },
    {
      "question_id": 3022,
      "content": "Which of the following distributions is characterized by a single parameter λ?",
      "type": "MCQ",
      "question_options": [
        "Normal",
        "Exponential",
        "Uniform",
        "Lognormal"
      ],
      "correct_answer": 1,
      "points": 8,
      "explanation": "The exponential distribution is characterized by a single rate parameter λ.",
      "course_id": 4,
      "week_id": 2,
      "lecture_id": 3,
      "status": "active",
      "tags": ["distributions", "parameters"]
    },
    {
      "question_id": 3023,
      "content": "In hypothesis testing, what is the probability of Type I error?",
      "type": "MCQ",
      "question_options": [
        "α",
        "β",
        "1-α",
        "1-β"
      ],
      "correct_answer": 0,
      "points": 5,
      "explanation": "The probability of Type I error (false positive) is equal to the significance level α.",
      "course_id": 4,
      "week_id": 2,
      "lecture_id": 4,
      "status": "active",
      "tags": ["hypothesis testing", "errors"]
    },
    {
      "question_id": 3024,
      "content": "What is the primary purpose of conducting market experiments in demand analysis?",
      "type": "MCQ",
      "question_options": [
        "To observe actual consumer behavior at different price points",
        "To reduce production costs",
        "To increase brand awareness",
        "To test employee performance"
      ],
      "correct_answer": 0,
      "points": 8,
      "explanation": "Market experiments help observe real consumer responses to different prices, providing data for demand curve estimation.",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["market experiments", "demand analysis"]
    },
    {
      "question_id": 3025,
      "content": "Which of the following transformations would linearize a constant elasticity demand model?",
      "type": "MCQ",
      "question_options": [
        "Logarithmic",
        "Square root",
        "Reciprocal",
        "Exponential"
      ],
      "correct_answer": 0,
      "points": 10,
      "explanation": "Taking logs of both price and quantity transforms the constant elasticity model into a linear form suitable for regression.",
      "course_id": 4,
      "week_id": 4,
      "lecture_id": 8,
      "status": "active",
      "tags": ["transformations", "elasticity"]
    }
  ],

  "assignments": [
    {
      "assignment_id": 3001,
      "week_id": 1,
      "title": "Data Visualization Fundamentals",
      "description": "Practice creating effective visualizations and interpreting graphical data representations.",
      "type": "practice",
      "due_date": "2025-05-10",
      "start_date": "2025-05-03",
      "is_published": true,
      "question_ids": [3001, 3002, 3010],
      "course_id": 4
    },
    {
      "assignment_id": 3002,
      "week_id": 3,
      "title": "Probability and Statistical Analysis",
      "description": "Apply probability concepts including joint/marginal probabilities, conditional probability, and Bayes' Rule.",
      "type": "practice",
      "due_date": "2025-05-17",
      "start_date": "2025-05-10",
      "is_published": true,
      "question_ids": [3003, 3004, 3005, 3009],
      "course_id": 4
    },
    {
      "assignment_id": 3003,
      "week_id": 3,
      "title": "Chi-square and Independence Testing",
      "description": "Practice testing for statistical independence using chi-square tests and interpreting results.",
      "type": "practice",
      "due_date": "2025-05-24",
      "start_date": "2025-05-17",
      "is_published": true,
      "question_ids": [3006, 3007, 3008],
      "course_id": 4
    },
    {
      "assignment_id": 3004,
      "week_id": 2,
      "title": "Distribution Fitting and Hypothesis Testing",
      "description": "Advanced exercises in distribution fitting, parameter estimation, and hypothesis testing.",
      "type": "graded",
      "due_date": "2025-05-31",
      "start_date": "2025-05-24",
      "is_published": true,
      "question_ids": [3013, 3014, 3015, 3017, 3022, 3023],
      "course_id": 4
    },
    {
      "assignment_id": 3005,
      "week_id": 4,
      "title": "Demand Analysis and Market Experiments",
      "description": "Comprehensive analysis of demand curves, price elasticity, and market experiment results.",
      "type": "graded",
      "due_date": "2025-06-07",
      "start_date": "2025-05-31",
      "is_published": true,
      "question_ids": [3011, 3012, 3016, 3018, 3019, 3020, 3021, 3024, 3025],
      "course_id": 4
    }
  ],
  
    "personal_resources": [
    {
        "resource_id": 1,
        "name": "Business Analytics Cheat Sheet",
        "description": "Comprehensive reference guide for core concepts in business analytics, including visualization methods, statistical techniques, and demand modeling.",
        "course_id": 4,
        "user_id": 2001,
        "is_active": true,
        "LLM_Summary": {
        "summary": "An extensive and practical cheat sheet covering key concepts in business analytics, including visualization techniques, distribution fitting methodologies, association analysis, Bayesian inference, and advanced demand modeling. It provides step-by-step procedures, statistical formulas, and practical applications of these methods in real-world business scenarios. Users can reference it for quick access to theoretical principles and implementation guidelines across core business analytics topics.",
        "concepts_covered": [
            "Data visualization best practices and chart selection",
            "Empirical and theoretical distribution fitting",
            "Parameter estimation (MLE) and goodness-of-fit tests",
            "Association rule mining and correlation metrics",
            "Bayes' theorem and Bayesian decision-making",
            "Demand curve estimation and price elasticity models",
            "Interpretation of Q-Q plots and statistical validation",
            "Experimental design for market data collection",
            "Linear and non-linear regression in business forecasting",
            "Evaluating model performance and practical case applications"
        ],
        "concepts_not_covered": [
            "Machine learning models for predictive analytics",
            "Advanced time-series forecasting",
            "Unsupervised learning techniques (e.g., clustering)",
            "Causal inference and advanced experimental designs",
            "Big data tools and technologies (e.g., Spark, Hadoop)"
        ]
        }
    }
    ],

    "personal_resource_files": [
    {
        "file_id": 1,
        "resource_id": 1,
        "name": "Visualization Examples",
        "type": "text",
        "content": "# Data Visualization Examples\n\n## Good Example\n- Clear purpose and audience focus\n- Minimal non-data ink and clutter-free presentation\n- Appropriate scaling and axis labeling\n- Consistent color usage for clarity\n\n## Bad Example\n- 3D effects that distort data interpretation\n- Misleading axis scales that manipulate perception\n- Overuse of chart junk (gridlines, labels, and annotations)\n- Inconsistent color schemes causing confusion\n\n## Chart Selection Guide\n- Bar Charts: Best for categorical comparisons\n- Line Charts: Ideal for time series and trends\n- Scatter Plots: Effective for relationships between two variables\n- Histograms: Suitable for displaying distributions\n\n## Visualization Pitfalls\n- Avoid cherry-picking data to mislead\n- Ensure axis and scale consistency\n- Use intuitive color schemes to enhance understanding",
        "file_type": "text/markdown",
        "file_size": 2048
    },
    {
        "file_id": 2,
        "resource_id": 1,
        "name": "Distribution Fitting Guide",
        "type": "file",
        "content": "Comprehensive guide to empirical and theoretical distribution fitting in business analytics. It covers key techniques for identifying appropriate probability distributions using exploratory data analysis and formal statistical tests. Includes detailed explanations of parameter estimation through Maximum Likelihood Estimation (MLE) and graphical validation using Q-Q plots. Discusses how to conduct goodness-of-fit tests such as the Chi-square and Kolmogorov-Smirnov tests, with examples of interpreting statistical outputs and their implications for business forecasting and decision-making.",
        "file_path": "/uploads/user2001/ba201/distribution_guide.pdf",
        "file_type": "application/pdf",
        "file_size": 4096
    },
    {
        "file_id": 3,
        "resource_id": 1,
        "name": "Bayesian Analysis Primer",
        "type": "file",
        "content": "Step-by-step guide to applying Bayesian analysis in business settings. This document covers prior probability estimation, likelihood calculation, and posterior inference. It includes real-world examples from marketing analytics, risk assessment, and quality control. Detailed derivations of Bayes' theorem and its use in decision-making under uncertainty are provided, along with methods for updating probabilities with new evidence.",
        "file_path": "/uploads/user2001/ba201/bayes_analysis.pdf",
        "file_type": "application/pdf",
        "file_size": 5120
    },
    {
        "file_id": 4,
        "resource_id": 1,
        "name": "Demand Curve Modeling Guide",
        "type": "file",
        "content": "An in-depth guide to advanced demand modeling techniques, including linear and non-linear demand response curves. Explores the mathematical foundations of demand elasticity, including constant elasticity models and log-log transformations. Describes how to conduct market experiments to estimate demand curves and interpret elasticity metrics for strategic pricing. Includes case studies illustrating how businesses use demand models to forecast revenue and optimize pricing decisions.",
        "file_path": "/uploads/user2001/ba201/demand_curve_guide.pdf",
        "file_type": "application/pdf",
        "file_size": 6144
    }
    ]


  }